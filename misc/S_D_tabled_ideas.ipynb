{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f0aa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T14:01:45.617689Z",
     "start_time": "2023-02-03T14:01:45.591409Z"
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd12691",
   "metadata": {},
   "source": [
    "# Combined datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05bef4",
   "metadata": {},
   "source": [
    "<table style=\"border: 0; border-collapse: collapse; border-spacing: 0;\">\n",
    "  <tr>\n",
    "    <td style=\"width:50%; border: none;\">\n",
    "    Problem: Data analysis frequently necessitates the manipulation of datasets scattered across various tables, such as those found in pandas DataFrames or SQL databases. The integration and examination of this distributed data mandate the execution of table joins via shared fields. Although this operation is not inherently difficult, it can become cumbersome when the datasets are extensive or when numerous tables are involved.\n",
    "    <p><p>\n",
    "    See more in discussion: <a href=\"https://github.com/i2mint/tabled/discussions/3\">https://github.com/i2mint/tabled/discussions/3</a>\n",
    "    </td>\n",
    "    <td style=\"border: none;\">\n",
    "      <img src=\"https://github.com/i2mint/tabled/assets/1906276/37dd2e30-3792-4d0b-94a5-c48e83986061\" width=320 alt=\"Venn diagram of fields\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69264267",
   "metadata": {},
   "source": [
    "## Making get_table_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29df2d",
   "metadata": {},
   "source": [
    "We want to make this:\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a6fbdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable, Mapping\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa08553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_table(tables, fields):\n",
    "    \"\"\"\n",
    "    Get table with requested `fields`, computed by joining relevant tables \n",
    "    of `tables`.\n",
    "    \"\"\"\n",
    "    resolution_sequence_ = resolution_sequence(tables, fields)\n",
    "    join_view = compute_join_resolution(resolution_sequence_, tables)\n",
    "    return join_view\n",
    "\n",
    "# where...\n",
    "\n",
    "def resolution_sequence(field_sets: dict, fields_to_cover: Iterable) -> list:\n",
    "    \"\"\"\n",
    "    Returns the list of join operations that, when carried out, \n",
    "    cover the given fields.\n",
    "    \n",
    "    :param field_sets: A mapping of table names to sets of their fields.\n",
    "    :param fields: The fields to cover.\n",
    "    \"\"\"\n",
    "\n",
    "def compute_join_resolution(\n",
    "        resolution_sequence: Iterable, tables: Mapping[str, pd.DataFrame]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carries `resolution_sequence` join operations out with tables taken \n",
    "    from `tables`.\n",
    "    \n",
    "    :param resolution_sequence: An iterable of join operations to carry out. \n",
    "        Each join operation is either a table name (str) or a JoinWith object.\n",
    "        If it's a JoinWith object, it's assumed that the table has already \n",
    "        been joined and the fields to remove are in the `remove` \n",
    "        attribute of the object.\n",
    "    :param tables: A mapping of table names to tables (pd.DataFrame)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42ba29d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 7.0.1 (20221109.1506)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"198pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 198.00 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-328 194,-328 194,4 -4,4\"/>\n",
       "<!-- resolution_sequence_ -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>resolution_sequence_</title>\n",
       "<text text-anchor=\"middle\" x=\"67\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">resolution_sequence_</text>\n",
       "</g>\n",
       "<!-- compute_join_resolution -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>compute_join_resolution</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"190,-108 38,-108 38,-72 190,-72 190,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"114\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">compute_join_resolution</text>\n",
       "</g>\n",
       "<!-- resolution_sequence_&#45;&gt;compute_join_resolution -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>resolution_sequence_&#45;&gt;compute_join_resolution</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M78.38,-144.05C83.67,-136.18 90.09,-126.62 96.01,-117.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.89,-119.79 101.56,-109.53 93.08,-115.88 98.89,-119.79\"/>\n",
       "</g>\n",
       "<!-- tables -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>tables</title>\n",
       "<text text-anchor=\"middle\" x=\"151\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">tables</text>\n",
       "</g>\n",
       "<!-- resolution_sequence -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>resolution_sequence</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"132.5,-252 5.5,-252 5.5,-216 132.5,-216 132.5,-252\"/>\n",
       "<text text-anchor=\"middle\" x=\"69\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">resolution_sequence</text>\n",
       "</g>\n",
       "<!-- tables&#45;&gt;resolution_sequence -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>tables&#45;&gt;resolution_sequence</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M131.15,-288.05C121.13,-279.5 108.78,-268.96 97.74,-259.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"100.27,-257.09 90.39,-253.26 95.72,-262.41 100.27,-257.09\"/>\n",
       "</g>\n",
       "<!-- tables&#45;&gt;compute_join_resolution -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>tables&#45;&gt;compute_join_resolution</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M152.6,-288.15C154.88,-258.4 157.37,-194.9 143,-144 140.5,-135.16 136.32,-126.16 131.89,-118.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"134.93,-116.41 126.83,-109.58 128.91,-119.97 134.93,-116.41\"/>\n",
       "</g>\n",
       "<!-- fields -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>fields</title>\n",
       "<text text-anchor=\"middle\" x=\"69\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">fields</text>\n",
       "</g>\n",
       "<!-- fields&#45;&gt;resolution_sequence -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>fields&#45;&gt;resolution_sequence</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69,-288.05C69,-280.68 69,-271.84 69,-263.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"72.5,-263.79 69,-253.79 65.5,-263.79 72.5,-263.79\"/>\n",
       "</g>\n",
       "<!-- resolution_sequence&#45;&gt;resolution_sequence_ -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>resolution_sequence&#45;&gt;resolution_sequence_</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M68.51,-215.7C68.29,-208.21 68.04,-199.26 67.8,-190.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"71.3,-191.03 67.52,-181.14 64.31,-191.23 71.3,-191.03\"/>\n",
       "</g>\n",
       "<!-- join_view -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>join_view</title>\n",
       "<text text-anchor=\"middle\" x=\"114\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">join_view</text>\n",
       "</g>\n",
       "<!-- compute_join_resolution&#45;&gt;join_view -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>compute_join_resolution&#45;&gt;join_view</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M114,-71.7C114,-64.21 114,-55.26 114,-46.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"117.5,-47.14 114,-37.14 110.5,-47.14 117.5,-47.14\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x115356990>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__import__('meshed').code_to_dag(combined_table).dot_digraph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "699a8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "tables = {\n",
    "    \"A\": pd.DataFrame({'b': [1, 2, 3, 33], 'c': [4, 5, 6, 66]}),\n",
    "    \"B\": pd.DataFrame(\n",
    "        {'b': [1, 2, 3], 'a': [4, 5, 6], 'd': [7, 8, 9], \n",
    "         'e': [10, 11, 12], 'f': [13, 14, 15]}\n",
    "    ),\n",
    "    \"C\": pd.DataFrame({'f': [13, 14, 15], 'g': [4, 5, 6]}),\n",
    "    \"D\": pd.DataFrame(\n",
    "        {'d': [7, 8, 77], 'e': [10, 11, 77], 'h': [7, 8, 9], 'i': [1, 2, 3]}\n",
    "    ),\n",
    "    \"E\": pd.DataFrame({'i': [1, 2, 3], 'j': [4, 5, 6]})\n",
    "}\n",
    "\n",
    "field_sets = {table_id: set(df.columns) for table_id, df in tables.items()}\n",
    "assert field_sets == {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class JoinWith:\n",
    "    table_key: str\n",
    "    remove: list = None\n",
    "\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "expected_join_resolution = [\n",
    "    'B',\n",
    "    JoinWith('C', remove=['a', 'f']),\n",
    "    JoinWith('D', remove=['d', 'e', 'h']),\n",
    "    JoinWith('E', remove=['i'])\n",
    "]\n",
    "expected_result = pd.DataFrame({\n",
    "    'b': [1, 2],\n",
    "    'g': [4, 5],\n",
    "    'j': [4, 5]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2d934",
   "metadata": {},
   "source": [
    "The tests would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b3b95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_join_resolution(\n",
    "        join_resolution: Callable,\n",
    "        *,\n",
    "        field_sets: dict = field_sets, \n",
    "        fields_to_cover: Iterable = fields_to_cover,\n",
    "        expected_join_resolution: list = expected_join_resolution,\n",
    "    ):\n",
    "    assert join_resolution(field_sets, fields_to_cover) == expected_join_resolution\n",
    "\n",
    "def test_compute_join_resolution(\n",
    "    compute_join_resolution: Callable, \n",
    "    *,\n",
    "    resolution_sequence: Iterable = expected_join_resolution,\n",
    "    tables: Mapping[str, pd.DataFrame] = tables,\n",
    "    expected_result: pd.DataFrame = expected_result,   \n",
    "):\n",
    "    result = compute_join_resolution(resolution_sequence, tables)\n",
    "    assert tables_are_equal(result, expected_result)\n",
    "\n",
    "\n",
    "def tables_are_equal(\n",
    "        t1, t2, *, ignore_index=True, sort_index=True, sort_columns=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Compare two tables for equality, ignoring (by default) index and column order.\n",
    "    \"\"\"\n",
    "    if ignore_index:\n",
    "        t1 = t1.reset_index(drop=True)\n",
    "        t2 = t2.reset_index(drop=True)\n",
    "    elif sort_index:\n",
    "        t1 = t1.sort_index(axis=1)\n",
    "        t2 = t2.sort_index(axis=1)\n",
    "    if sort_columns:\n",
    "        t1 = t1.sort_index(axis=0)\n",
    "        t2 = t2.sort_index(axis=0)\n",
    "    return t1.equals(t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0112fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_join_op(obj):\n",
    "    if not isinstance(obj, JoinWith):\n",
    "        return JoinWith(obj)\n",
    "    return obj\n",
    "        \n",
    "\n",
    "def compute_join_resolution(\n",
    "        resolution_sequence: Iterable, tables: Mapping[str, pd.DataFrame]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carries `resolution_sequence` join operations out with tables taken from `tables`.\n",
    "    \n",
    "    :param resolution_sequence: An iterable of join operations to carry out. \n",
    "        Each join operation is either a table name (str) or a JoinWith object.\n",
    "        If it's a JoinWith object, it's assumed that the table has already been joined\n",
    "        and the fields to remove are in the `remove` attribute of the object.\n",
    "    :param tables: A mapping of table names to tables (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    join_ops = map(ensure_join_op, resolution_sequence)\n",
    "    table_key = next(join_ops).table_key\n",
    "    joined = tables[table_key]\n",
    "    for join_op in join_ops:\n",
    "        table = tables[join_op.table_key]\n",
    "        joined = joined.merge(table, how='inner')\n",
    "        if join_op.remove:\n",
    "            remove_cols = set(join_op.remove) & set(joined.columns)\n",
    "            joined = joined.drop(columns=remove_cols)\n",
    "    return joined\n",
    "\n",
    "# from tabled.multi import compute_join_resolution, ensure_join_op\n",
    "\n",
    "# test_compute_join_resolution(compute_join_resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32fb5b4",
   "metadata": {},
   "source": [
    "**TODO: Implement the join_resolution function -- possibly using the [intersection_graph](https://github.com/i2mint/tabled/blob/282784fc4113b47371ce59a6e14ad2566d7f2abb/tabled/util.py#L25) function on field_sets to help**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834f497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d44a722",
   "metadata": {},
   "source": [
    "## Getting the minimum connected set covering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75b6514f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('labeled_sets',\n",
       " {'A': {'b', 'c'},\n",
       "  'B': {'a', 'b', 'd', 'e', 'f'},\n",
       "  'C': {'f', 'g'},\n",
       "  'D': {'d', 'e', 'h', 'i'},\n",
       "  'E': {'i', 'j'}},\n",
       " 'sets_containing_elements',\n",
       " {'b': {'A', 'B'},\n",
       "  'c': {'A'},\n",
       "  'f': {'B', 'C'},\n",
       "  'a': {'B'},\n",
       "  'd': {'B', 'D'},\n",
       "  'e': {'B', 'D'},\n",
       "  'g': {'C'},\n",
       "  'h': {'D'},\n",
       "  'i': {'D', 'E'},\n",
       "  'j': {'E'}},\n",
       " 'intersections',\n",
       " {'A': {'B': {'b'}},\n",
       "  'B': {'A': {'b'}, 'C': {'f'}, 'D': {'d', 'e'}},\n",
       "  'C': {'B': {'f'}},\n",
       "  'D': {'B': {'d', 'e'}, 'E': {'i'}},\n",
       "  'E': {'D': {'i'}}},\n",
       " 'graph',\n",
       " {'A': ['B'], 'B': ['A', 'C', 'D'], 'C': ['B'], 'D': ['B', 'E'], 'E': ['D']})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabled.util import (\n",
    "    intersection_graph, invert_labeled_collection, map_values, breadth_first_traversal\n",
    ")\n",
    "\n",
    "labeled_sets = {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "\n",
    "sets_containing_elements = invert_labeled_collection(labeled_sets, values_container=set)\n",
    "intersections = intersection_graph(labeled_sets, edge_labels='elements')\n",
    "graph = {k: list(v) for k, v in intersections.items()}\n",
    "\n",
    "(\n",
    "    'labeled_sets', labeled_sets, \n",
    "    'sets_containing_elements', sets_containing_elements, \n",
    "    'intersections', intersections, \n",
    "    'graph', graph\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0745a3",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/i2mint/tabled/assets/1906276/37dd2e30-3792-4d0b-94a5-c48e83986061\" width=320 alt=\"Venn diagram of fields\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc7ba4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 'A'), ('B', 'C'), ('B', 'D'), ('D', 'E')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import KT, VT\n",
    "\n",
    "def minimum_covering_tree(\n",
    "        graph: Mapping[KT, Iterable], \n",
    "        target_subset: Iterable[VT],\n",
    "        start_node=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if start_node is None:  # if not start_node is given\n",
    "        start_node = next(iter(graph))  # take the first node\n",
    "    target_sub_set = set(target_subset)\n",
    "    traversal = breadth_first_traversal(graph, 'B', yield_edges=True)\n",
    "\n",
    "    edges = list()\n",
    "    covered = set()\n",
    "\n",
    "    for set_label_1, set_label_2 in traversal:\n",
    "        if target_sub_set.issubset(covered):\n",
    "            break\n",
    "        covered |= labeled_sets[set_label_1] | labeled_sets[set_label_2]\n",
    "        edges.append((set_label_1, set_label_2))\n",
    "\n",
    "    return edges\n",
    "\n",
    "\n",
    "minimum_covering_tree(graph, {'b', 'g', 'j'}, 'B')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b30b02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sub_set = {'b', 'g', 'j'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36908758",
   "metadata": {},
   "source": [
    "* for each leaf edge, recursively remove the leaf edge iff none of the lost associated elements are in target subset\n",
    "* now go through each edge (possible starting with leaf), yielding `Join(table_id)` for each of the two `table_id` of the edge (as long as it hasn't been joined before)\n",
    "* now go through the resulting `Join` sequence and weave `Remove` commands in them with the following rule: Remove field if the associated elements (fields) are neither in target field nor is the field associated with any of the later Joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dbbec7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 'A'), ('B', 'C'), ('D', 'E')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple, Set\n",
    "\n",
    "def get_leaf_edges(\n",
    "        graph: Mapping[KT, Iterable[VT]], \n",
    "        target_subset: Set[VT],\n",
    "        start_node: KT = None\n",
    "    ) -> List[Tuple[KT, KT]]:\n",
    "    if start_node is None:  # if no start_node is given\n",
    "        start_node = next(iter(graph))  # take the first node\n",
    "    \n",
    "    traversal = breadth_first_traversal(graph, start_node, yield_edges=True)\n",
    "\n",
    "    edges = []  # List to store edges of the covering tree\n",
    "    covered = set()  # Set to store covered elements\n",
    "    node_connections = {}  # Dictionary to track the number of connections for each node\n",
    "\n",
    "    for node1, node2 in traversal:\n",
    "        if target_subset.issubset(covered):\n",
    "            break\n",
    "\n",
    "        # Update covered elements and the number of connections for each node\n",
    "        covered |= labeled_sets[node1] | labeled_sets[node2]\n",
    "        node_connections[node1] = node_connections.get(node1, 0) + 1\n",
    "        node_connections[node2] = node_connections.get(node2, 0) + 1\n",
    "        edges.append((node1, node2))\n",
    "\n",
    "    # Filter edges to return only those that lead to a node with one connection\n",
    "    leaf_edges = [edge for edge in edges if node_connections[edge[1]] == 1]\n",
    "\n",
    "    return leaf_edges\n",
    "\n",
    "leaf_edges = get_leaf_edges(graph, {'b', 'g', 'j'}, 'B')\n",
    "leaf_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc29cf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'A': [], 'B': ['C', 'D'], 'C': ['B'], 'D': ['B', 'E'], 'E': ['D']},\n",
       " [('B', 'C'), ('D', 'E')])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_leaf_edges_after_removal(\n",
    "    graph: Mapping[str, List[str]],\n",
    "    labeled_sets: Mapping[str, Set[str]],\n",
    "    target_sub_set: Set[str],\n",
    "    current_leaf_edges: List[Tuple[str, str]]\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Update the list of leaf edges after removing an edge, ensuring that the resulting\n",
    "    leaf edges do not lead to the loss of any elements in the target subset.\n",
    "\n",
    "    :param graph: The graph represented as an adjacency list.\n",
    "    :param labeled_sets: The sets of elements labeled by nodes.\n",
    "    :param target_subset: The target subset of elements that must remain covered.\n",
    "    :param current_leaf_edges: The current list of leaf edges.\n",
    "    :return: An updated list of leaf edges.\n",
    "    \"\"\"\n",
    "    new_leaf_edges = []\n",
    "    for node1, node2 in current_leaf_edges:\n",
    "        # Use symmetric difference to consider the edge as undirected\n",
    "        unique_elements = ((labeled_sets[node1] - labeled_sets[node2]) | \n",
    "                           (labeled_sets[node2] - labeled_sets[node1])) & target_sub_set\n",
    "        \n",
    "        # Check if the unique elements do not include any from the target subset\n",
    "        if not unique_elements:\n",
    "            # Remove the edge from the graph\n",
    "            graph[node1].remove(node2)\n",
    "            graph[node2].remove(node1)\n",
    "        else:\n",
    "            # If the edge cannot be removed, add it to the new list of leaf edges\n",
    "            new_leaf_edges.append((node1, node2))\n",
    "\n",
    "    # Recalculate the leaf edges after removals\n",
    "    for node, neighbors in graph.items():\n",
    "        if len(neighbors) == 1:  # If the node is now a leaf node\n",
    "            neighbor = neighbors[0]\n",
    "            if (node, neighbor) not in new_leaf_edges and (neighbor, node) not in new_leaf_edges:\n",
    "                new_leaf_edges.append((node, neighbor))\n",
    "\n",
    "    return new_leaf_edges\n",
    "\n",
    "# Create a copy of the original graph to modify\n",
    "modifiable_graph = {node: neighbors[:] for node, neighbors in graph.items()}\n",
    "\n",
    "# Update the leaf edges considering the target subset\n",
    "updated_leaf_edges = update_leaf_edges_after_removal(\n",
    "    modifiable_graph, labeled_sets, target_sub_set, leaf_edges)\n",
    "\n",
    "# The updated graph and leaf edges\n",
    "(modifiable_graph, updated_leaf_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e222a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Tuple, Dict\n",
    "\n",
    "class Join:\n",
    "    def __init__(self, table_id: str, remove: List[str] = None):\n",
    "        self.table_id = table_id\n",
    "        self.remove = remove or []\n",
    "\n",
    "    def __repr__(self):\n",
    "        remove_str = f\", remove={self.remove}\" if self.remove else \"\"\n",
    "        return f\"Join('{self.table_id}'{remove_str})\"\n",
    "\n",
    "def determine_remove_fields(\n",
    "    labeled_sets: Dict[str, Set[str]],\n",
    "    target_sub_set: Set[str],\n",
    "    joined_tables: Set[str],\n",
    "    current_table: str\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Determine which fields should be removed for a given table\n",
    "\n",
    "    :param labeled_sets: The sets of elements labeled by nodes\n",
    "    :param target_sub_set: The target subset of elements that must remain covered\n",
    "    :param joined_tables: The set of tables that have been or will be joined\n",
    "    :param current_table: The current table being processed\n",
    "    :return: A list of fields to remove\n",
    "    \"\"\"\n",
    "    fields = labeled_sets[current_table]\n",
    "    removable_fields = [\n",
    "        field for field in fields\n",
    "        if field not in target_sub_set\n",
    "        and all(field not in labeled_sets[table] for table in joined_tables if table != current_table)\n",
    "    ]\n",
    "    return removable_fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0f82247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', Join('C', remove=['f', 'a']), Join('D', remove=['h', 'd', 'e']), Join('E', remove=['i'])]\n"
     ]
    }
   ],
   "source": [
    "def generate_join_sequence(\n",
    "    leaf_edges: List[Tuple[str, str]],\n",
    "    labeled_sets: Dict[str, Set[str]],\n",
    "    target_sub_set: Set[str]\n",
    ") -> List[Join]:\n",
    "    \"\"\"\n",
    "    Generate a sequence of joins with remove commands based on leaf edges\n",
    "\n",
    "    :param leaf_edges: The list of leaf edges to process\n",
    "    :param labeled_sets: The sets of elements labeled by nodes\n",
    "    :param target_sub_set: The target subset of elements that must remain covered\n",
    "    :return: A list of Join operations\n",
    "    \"\"\"\n",
    "    joins = []  # Stores the sequence of joins\n",
    "    joined_tables = set()  # Tracks tables that have been joined\n",
    "    future_joins = {node for edge in leaf_edges for node in edge}  # Tracks all future joins\n",
    "\n",
    "    # Start with the first node of the first edge without removals\n",
    "    first_node = leaf_edges[0][0]\n",
    "    first_node_remove_fields = determine_remove_fields(\n",
    "        labeled_sets, target_sub_set, future_joins, first_node\n",
    "    )\n",
    "    joins.append(first_node)\n",
    "\n",
    "    for node1, node2 in leaf_edges:\n",
    "        for node in (node1, node2):\n",
    "            if node not in joined_tables:\n",
    "                # Determine the fields to remove considering future joins\n",
    "                remove_fields = determine_remove_fields(\n",
    "                    labeled_sets, target_sub_set, future_joins, node\n",
    "                )\n",
    "                if node == first_node:\n",
    "                    remove_fields = first_node_remove_fields\n",
    "                elif node in leaf_edges[0]:  # If the node is to join with the first node\n",
    "                    remove_fields.extend(first_node_remove_fields)  # Append remove_fields from the first node\n",
    "\n",
    "                if node != first_node:\n",
    "                    joins.append(Join(node, remove_fields))\n",
    "                # Update the tracking sets\n",
    "                joined_tables.add(node)\n",
    "                future_joins.remove(node)\n",
    "\n",
    "    return joins\n",
    "\n",
    "# Using the leaf edges and labeled sets from the previous example\n",
    "target_sub_set = {'b', 'g', 'j'}\n",
    "leaf_edges = [('B', 'C'), ('D', 'E')]\n",
    "\n",
    "# Generate the join sequence\n",
    "join_sequence = generate_join_sequence(leaf_edges, labeled_sets, target_sub_set)\n",
    "print(join_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5401dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_join_op(obj):\n",
    "    if not isinstance(obj, Join):\n",
    "        return Join(obj)\n",
    "    return obj\n",
    "        \n",
    "\n",
    "def compute_join_resolution(\n",
    "        resolution_sequence: Iterable, tables: Mapping[str, pd.DataFrame]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carries `resolution_sequence` join operations out with tables taken from `tables`.\n",
    "    \n",
    "    :param resolution_sequence: An iterable of join operations to carry out. \n",
    "        Each join operation is either a table name (str) or a Join object.\n",
    "        If it's a Join object, it's assumed that the table has already been joined\n",
    "        and the fields to remove are in the `remove` attribute of the object.\n",
    "    :param tables: A mapping of table names to tables (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    join_ops = map(ensure_join_op, resolution_sequence)\n",
    "    table_key = next(join_ops).table_id\n",
    "    joined = tables[table_key]\n",
    "    for join_op in join_ops:\n",
    "        table = tables[join_op.table_id]\n",
    "        joined = joined.merge(table, how='inner')\n",
    "        if join_op.remove:\n",
    "            remove_cols = set(join_op.remove) & set(joined.columns)\n",
    "            joined = joined.drop(columns=remove_cols)\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2c966ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>g</th>\n",
       "      <th>j</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   b  g  j\n",
       "0  1  4  4\n",
       "1  2  5  5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_join_resolution(join_sequence,tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f6827",
   "metadata": {},
   "source": [
    "# WIP Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f15a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fdbca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e2c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b75fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cfeb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "38d7069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 'B'), ('B', 'A'), ('B', 'D'), ('D', 'E')]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def approximate_steiner_tree(graph, terminal_sets):\n",
    "    # Flatten the set of terminal nodes\n",
    "    terminal_nodes = set.union(*terminal_sets)\n",
    "\n",
    "    # Create a complete graph of terminals with shortest path distances as weights\n",
    "    metric_closure = nx.algorithms.tree.mst.metric_closure(graph, weight='weight')\n",
    "    mst = nx.minimum_spanning_tree(metric_closure, weight='weight')\n",
    "    \n",
    "    # Map the tree back to the original graph\n",
    "    steiner_tree = nx.Graph()\n",
    "    for (u, v) in mst.edges():\n",
    "        path = nx.shortest_path(graph, source=u, target=v, weight='weight')\n",
    "        nx.add_path(steiner_tree, path)\n",
    "\n",
    "    return steiner_tree\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "def find_terminals(labels, labels_to_cover):\n",
    "    \"\"\"Identify terminal nodes based on labels.\"\"\"\n",
    "    terminals = set()\n",
    "    for node, node_labels in labels.items():\n",
    "        if labels_to_cover.intersection(node_labels):\n",
    "            terminals.add(node)\n",
    "    return terminals\n",
    "\n",
    "def construct_graph(adjacencies):\n",
    "    \"\"\"Construct a graph from adjacency list.\"\"\"\n",
    "    graph = nx.Graph()\n",
    "    for node, neighbors in adjacencies.items():\n",
    "        for neighbor in neighbors:\n",
    "            graph.add_edge(node, neighbor)\n",
    "    return graph\n",
    "\n",
    "def construct_complete_graph(graph, terminals):\n",
    "    \"\"\"Construct a complete graph between terminal nodes with shortest path distances.\"\"\"\n",
    "    complete_graph = nx.Graph()\n",
    "    for node1 in terminals:\n",
    "        for node2 in terminals:\n",
    "            if node1 != node2:\n",
    "                distance = nx.shortest_path_length(graph, source=node1, target=node2)\n",
    "                complete_graph.add_edge(node1, node2, weight=distance)\n",
    "    return complete_graph\n",
    "\n",
    "def steiner_tree_from_complete(complete_graph, terminals):\n",
    "    \"\"\"Find a Steiner tree from the complete graph.\"\"\"\n",
    "    mst = nx.minimum_spanning_tree(complete_graph, weight='weight')\n",
    "    steiner_tree = nx.Graph()\n",
    "    for (u, v) in mst.edges():\n",
    "        path = nx.shortest_path(graph, source=u, target=v)\n",
    "        nx.add_path(steiner_tree, path)\n",
    "    return steiner_tree\n",
    "\n",
    "# Input data\n",
    "adjacencies = {'A': ['B'], 'B': ['A', 'C', 'D'], 'C': ['B'], 'D': ['B', 'E'], 'E': ['D']}\n",
    "labels = {'A': {'b', 'c'}, 'B': {'a', 'b', 'd', 'e', 'f'}, 'C': {'f', 'g'}, 'D': {'d', 'e', 'h', 'i'}, 'E': {'i', 'j'}}\n",
    "labels_to_cover = {'b', 'g', 'j'}\n",
    "\n",
    "# Process\n",
    "graph = construct_graph(adjacencies)\n",
    "terminals = find_terminals(labels, labels_to_cover)\n",
    "complete_graph = construct_complete_graph(graph, terminals)\n",
    "steiner_tree = steiner_tree_from_complete(complete_graph, terminals)\n",
    "\n",
    "# Result\n",
    "tree_edges = list(steiner_tree.edges())\n",
    "print(tree_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd7ba7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a74b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ced6884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', JoinWith(table_key='C', remove=[]), JoinWith(table_key='E', remove=[])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_join_resolution(\n",
    "        join_resolution: Callable,\n",
    "        *,\n",
    "        field_sets: dict = field_sets, \n",
    "        fields_to_cover: Iterable = fields_to_cover,\n",
    "        expected_join_resolution: list = expected_join_resolution,\n",
    "    ):\n",
    "    assert join_resolution(field_sets, fields_to_cover) == expected_join_resolution\n",
    "\n",
    "\n",
    "# Given test data and expected results\n",
    "field_sets = {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "\n",
    "\n",
    "def join_resolution(field_sets: dict, fields_to_cover: Iterable[str]) -> list:\n",
    "    fields_to_cover = set(fields_to_cover)\n",
    "    field_sets = field_sets.copy()\n",
    "    resolution_sequence = []\n",
    "    # Initialize covered fields with an empty set\n",
    "    covered_fields = set()\n",
    "\n",
    "    # Loop until we've covered all fields\n",
    "    while not fields_to_cover.issubset(covered_fields):\n",
    "        # Find the table that has the most uncovered fields to cover\n",
    "        next_table = max(\n",
    "            field_sets.keys(),\n",
    "            key=lambda t: len(fields_to_cover.intersection(field_sets[t])) - len(covered_fields.intersection(field_sets[t])),\n",
    "            default=None\n",
    "        )\n",
    "\n",
    "        if next_table is None:\n",
    "            # If we can't find a next table, it means we can't cover the fields with the given field sets\n",
    "            raise ValueError(\"Cannot cover all fields with the given field sets.\")\n",
    "\n",
    "        # Determine which fields from the new table we will need to remove after the join\n",
    "        fields_to_remove = (covered_fields & field_sets[next_table]) - fields_to_cover\n",
    "\n",
    "        # If this is not the first table, we create a JoinWith instance, otherwise, just add the table name\n",
    "        if resolution_sequence:\n",
    "            resolution_sequence.append(JoinWith(table_key=next_table, remove=list(fields_to_remove)))\n",
    "        else:\n",
    "            resolution_sequence.append(next_table)\n",
    "\n",
    "        # Update covered fields\n",
    "        covered_fields.update(field_sets[next_table])\n",
    "\n",
    "        # Remove the used table from field_sets to prevent reusing it\n",
    "        del field_sets[next_table]\n",
    "\n",
    "    return resolution_sequence\n",
    "\n",
    "# test_join_resolution(join_resolution)\n",
    "join_resolution(field_sets, fields_to_cover)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8480d4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'B')}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given test data and expected results\n",
    "field_sets = {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "\n",
    "def test_minimum_covering_intersections(\n",
    "        minimum_covering_intersections: Callable,\n",
    "        *,\n",
    "        field_sets: dict = field_sets, \n",
    "        fields_to_cover: Iterable = fields_to_cover,\n",
    "        expected_result: set = {('B', 'C'), ('B', 'D'), ('B', 'E')}\n",
    "    ):\n",
    "    assert minimum_covering_intersections(field_sets, fields_to_cover) == expected_result\n",
    "\n",
    "\n",
    "def minimum_covering_intersections(field_sets, fields_to_cover):\n",
    "    \"\"\"Returns a set of set pairs that connect the elements of `fields_to_cover`.\n",
    "    \n",
    "    :param field_sets: A mapping of sets of field names\n",
    "    :param fields_to_cover: A set of field names to cover\n",
    "    :return: A set of set pairs that connect the elements of `fields_to_cover`\n",
    "    \n",
    "    The returned pairs are `(key_1, key_2)` pairs of keys of `field_sets`\n",
    "    such that field_sets[key_1] & field_sets[key_2] is not empty.\n",
    "    The union of all the sets in the pairs should cover `fields_to_cover`.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "def minimum_covering_intersections(field_sets, fields_to_cover):\n",
    "    fields_to_cover = set(fields_to_cover)  # Ensure we have a set to work with\n",
    "    covering_pairs = set()\n",
    "    covered_fields = set()\n",
    "\n",
    "    # Go through all unique pairs of keys from field_sets\n",
    "    for key1, key2 in combinations(field_sets.keys(), 2):\n",
    "        intersection = field_sets[key1] & field_sets[key2]\n",
    "        \n",
    "        # Check if this pair covers any new fields that we need to cover\n",
    "        newly_covered = intersection & fields_to_cover - covered_fields\n",
    "        if newly_covered:\n",
    "            covering_pairs.add((key1, key2))\n",
    "            covered_fields.update(newly_covered)\n",
    "            \n",
    "        # If we've covered all fields, we can stop early\n",
    "        if covered_fields == fields_to_cover:\n",
    "            break\n",
    "\n",
    "    return covering_pairs\n",
    "\n",
    "minimum_covering_intersections(field_sets, fields_to_cover)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2b34823d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {'b', 'c'},\n",
       " 'B': {'a', 'b', 'd', 'e', 'f'},\n",
       " 'C': {'f', 'g'},\n",
       " 'D': {'d', 'e', 'h', 'i'},\n",
       " 'E': {'i', 'j'}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "329b9eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabled.util import intersection_graph\n",
    "\n",
    "assert intersection_graph(field_sets) == {\n",
    "    'A': {'B'}, 'B': {'A', 'C', 'D'}, 'C': {'B'}, 'D': {'B', 'E'}, 'E': {'D'}\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17584ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Tuple, Set\n",
    "\n",
    "def minimum_covering_intersections(field_sets: Dict[str, Set[str]], fields_to_cover: Set[str]) -> Set[Tuple[str, str]]:\n",
    "    # Construct the intersection graph\n",
    "    graph = intersection_graph(field_sets, edge_labels='elements')\n",
    "    \n",
    "    # Initialize variables to track covered fields and the pairs to return\n",
    "    covered_fields = set()\n",
    "    covering_pairs = set()\n",
    "\n",
    "    # Function to perform Depth-First Search (DFS) on the graph\n",
    "    def dfs(node, visited, component):\n",
    "        visited.add(node)\n",
    "        component.add(node)\n",
    "        for neighbour in graph[node]:\n",
    "            if neighbour not in visited:\n",
    "                dfs(neighbour, visited, component)\n",
    "                \n",
    "    # Find all connected components in the graph\n",
    "    visited = set()\n",
    "    components = []\n",
    "    for node in graph:\n",
    "        if node not in visited:\n",
    "            component = set()\n",
    "            dfs(node, visited, component)\n",
    "            components.append(component)\n",
    "    \n",
    "    # Find the minimum set of set pairs for each component\n",
    "    for component in components:\n",
    "        # Check if this component contains any of the fields we need to cover\n",
    "        if any(field in covered_fields for node in component for field in field_sets[node]):\n",
    "            # Perform a modified Prim's algorithm to find the minimum spanning tree that covers fields_to_cover\n",
    "            mst_edges = set()  # This will store the edges of the minimum spanning tree\n",
    "            component_fields_to_cover = {field for node in component for field in field_sets[node] if field in fields_to_cover}\n",
    "            covered_in_component = set()\n",
    "            available_edges = [(len(graph[node][neighbour]), node, neighbour) for node in component for neighbour in graph[node] if neighbour in component]\n",
    "            sorted_edges = sorted(available_edges)\n",
    "            \n",
    "            while component_fields_to_cover - covered_in_component:\n",
    "                # Select the edge with minimum weight (smallest intersection)\n",
    "                _, node1, node2 = sorted_edges.pop(0)\n",
    "                # Add this edge if it helps to cover more fields\n",
    "                if field_sets[node1] & field_sets[node2] & fields_to_cover:\n",
    "                    mst_edges.add((node1, node2))\n",
    "                    covered_in_component.update(field_sets[node1] & field_sets[node2])\n",
    "            \n",
    "            # Add the edges of this minimum spanning tree to the covering pairs\n",
    "            covering_pairs.update(mst_edges)\n",
    "    \n",
    "    # Make sure the pairs are in the correct order\n",
    "    ordered_covering_pairs = {(min(pair), max(pair)) for pair in covering_pairs}\n",
    "    \n",
    "    return ordered_covering_pairs\n",
    "\n",
    "\n",
    "field_sets = {'A': {'b', 'c'},\n",
    " 'B': {'a', 'b', 'd', 'e', 'f'},\n",
    " 'C': {'f', 'g'},\n",
    " 'D': {'d', 'e', 'h', 'i'},\n",
    " 'E': {'i', 'j'}}\n",
    "\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "\n",
    "edges = minimum_covering_intersections(field_sets, fields_to_cover)\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0b8683f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m edges \u001b[38;5;241m=\u001b[39m minimum_covering_intersections(field_sets, fields_to_cover)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(edges) \u001b[38;5;241m==\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "edges = minimum_covering_intersections(field_sets, fields_to_cover)\n",
    "assert sorted(edges) == [('B', 'C'), ('B', 'D'), ('D', 'E')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74063324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e32a1d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " JoinWith(table_key='C', remove=['f', 'c', 'b']),\n",
       " JoinWith(table_key='E', remove=['g', 'i', 'f', 'b', 'c'])]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterable, Mapping\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "# Define the JoinWith dataclass\n",
    "@dataclass\n",
    "class JoinWith:\n",
    "    table_key: str\n",
    "    remove: list = None\n",
    "\n",
    "# Define the function to find the join resolution\n",
    "def join_resolution(field_sets: dict, fields_to_cover: Iterable[str]) -> list:\n",
    "    fields_to_cover = set(fields_to_cover)  # Set of fields we need to cover\n",
    "    join_sequence = []  # The output sequence of joins\n",
    "    covered_fields = set()  # Fields covered by the join sequence so far\n",
    "\n",
    "    # While there are fields left to cover, iterate\n",
    "    while fields_to_cover:\n",
    "        # Find the table that covers the most fields which are not yet covered\n",
    "        next_table, next_fields = max(\n",
    "            field_sets.items(),\n",
    "            key=lambda item: len(fields_to_cover.intersection(item[1])) - len(covered_fields.intersection(item[1])),\n",
    "            default=(None, None)\n",
    "        )\n",
    "\n",
    "        # If no table can cover any more fields, break the loop\n",
    "        if not next_table:\n",
    "            break\n",
    "\n",
    "        # Determine which fields we will need to remove after joining this table\n",
    "        fields_to_remove = (covered_fields | next_fields) - fields_to_cover\n",
    "\n",
    "        # Add the next table to the join sequence\n",
    "        if join_sequence:\n",
    "            join_sequence.append(JoinWith(next_table, remove=list(fields_to_remove)))\n",
    "        else:  # If this is the first table, we don't need to remove any fields\n",
    "            join_sequence.append(next_table)\n",
    "\n",
    "        # Update the sets of covered fields and the remaining fields to cover\n",
    "        covered_fields.update(next_fields)\n",
    "        fields_to_cover -= next_fields\n",
    "\n",
    "        # Remove the chosen table from field_sets to avoid re-selecting it\n",
    "        del field_sets[next_table]\n",
    "\n",
    "    # Ensure that the last operation does not include a removal of fields from the last table joined\n",
    "    if join_sequence and isinstance(join_sequence[-1], JoinWith):\n",
    "        join_sequence[-1].remove = [f for f in join_sequence[-1].remove if f in covered_fields - fields_to_cover]\n",
    "\n",
    "    return join_sequence\n",
    "\n",
    "# Given test data and expected results\n",
    "field_sets = {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "\n",
    "# Expected output\n",
    "expected_join_resolution = [\n",
    "    'B',\n",
    "    JoinWith('C', remove=['a', 'f']),\n",
    "    JoinWith('D', remove=['d', 'e', 'h']),\n",
    "    JoinWith('E', remove=['i'])\n",
    "]\n",
    "\n",
    "# Run the function with the test data\n",
    "join_resolution_output = join_resolution(field_sets.copy(), fields_to_cover)\n",
    "\n",
    "# Display the output\n",
    "join_resolution_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017edbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "370c5715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabled.multi import Join, Remove\n",
    "\n",
    "\n",
    "def execute_commands(\n",
    "    commands: Iterable, \n",
    "    tables: Mapping[str, pd.DataFrame]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carries `commands` operations out with tables taken from `tables`.\n",
    "\n",
    "    :param commands: An iterable of join operations to carry out.\n",
    "        Each join operation is either a table name (str) or a JoinWith object.\n",
    "        If it's a JoinWith object, it's assumed that the table has already been joined\n",
    "        and the fields to remove are in the `remove` attribute of the object.\n",
    "    :param tables: A mapping of table names to tables (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    # join_ops = map(ensure_join_op, resolution_sequence)\n",
    "    commands = iter(commands)\n",
    "    first_command = next(commands)\n",
    "    assert isinstance(first_command, Join)\n",
    "    table_key = first_command.table_key\n",
    "    cumul = tables[table_key]  # initialize my accumulator\n",
    "    for command in commands:\n",
    "        if isinstance(command, Join):\n",
    "            table = tables[command.table_key]\n",
    "            cumul = cumul.merge(table, how='inner')\n",
    "        elif isinstance(command, Remove):\n",
    "            cumul = cumul.drop(columns=command.fields)\n",
    "        else:\n",
    "            raise TypeError(f'Unknown command type: {type(command)}')\n",
    "    return cumul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2790a66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {'b', 'c'},\n",
       " 'B': {'a', 'b', 'd', 'e', 'f'},\n",
       " 'C': {'f', 'g'},\n",
       " 'D': {'d', 'e', 'h', 'i'},\n",
       " 'E': {'i', 'j'}}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "dd7b8c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>g</th>\n",
       "      <th>j</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   b  g  j\n",
       "0  1  4  4\n",
       "1  2  5  5"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabled.multi import Join, Remove\n",
    "\n",
    "# commands equivalent to\n",
    "# ['A',\n",
    "#  JoinWith(table_key='C', remove=['f', 'c', 'b']),\n",
    "#  JoinWith(table_key='E', remove=['g', 'i', 'f', 'b', 'c'])]\n",
    "# expected_join_resolution = [\n",
    "#     'B',\n",
    "#     JoinWith('C', remove=['a', 'f']),\n",
    "#     JoinWith('D', remove=['d', 'e', 'h']),\n",
    "#     JoinWith('E', remove=['i'])\n",
    "# ]\n",
    "\n",
    "commands = [\n",
    "    Join('B'),\n",
    "    Join('C'),\n",
    "    Remove(['a', 'f']),\n",
    "    Join('D'),\n",
    "    Remove(['d', 'e', 'h']),\n",
    "    Join('E'),\n",
    "    Remove(['i']),\n",
    "]\n",
    "\n",
    "execute_commands(commands, tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f1848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc323b3e",
   "metadata": {},
   "source": [
    "# Proposal: tiny light framework for routing with mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0acf755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T12:07:39.913553Z",
     "start_time": "2023-02-06T12:07:39.875060Z"
    }
   },
   "outputs": [],
   "source": [
    "from i2.routing_forest import KeyFuncMapping\n",
    "from tabled import dflt_ext_mapping, get_ext\n",
    "\n",
    "from dol import Files, FilesOfZip, Pipe, wrap_kvs\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import io\n",
    "\n",
    "ext = KeyFuncMapping(\n",
    "    dict(\n",
    "        dflt_ext_mapping, \n",
    "        json=Pipe(io.BytesIO, partial(pd.read_json, orient='index'))\n",
    "    ),\n",
    "    get_ext,\n",
    ")\n",
    "\n",
    "table_trans = wrap_kvs(postget=lambda k, v: ext[k](v))\n",
    "\n",
    "TableFiles = table_trans(Files)\n",
    "TableZipFiles = table_trans(FilesOfZip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cacd8921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T12:07:44.075672Z",
     "start_time": "2023-02-06T12:07:43.973256Z"
    }
   },
   "outputs": [],
   "source": [
    "s = TableZipFiles('/Users/thorwhalen/Dropbox/_odata/sound/induction_motor_data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d016bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T12:07:45.572475Z",
     "start_time": "2023-02-06T12:07:45.547063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3909"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3d28ae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T14:50:43.104488Z",
     "start_time": "2023-02-03T14:50:43.053751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k='induction_motor_data/Bearing/Bearing_1250rpm/2021_01_27_15_03_06.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dataType</th>\n",
       "      <td>completeSample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deviceId</th>\n",
       "      <td>00000781O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flux</th>\n",
       "      <td>[31.3639, 23.8975, 35.5119, 27.0085, 16.6385, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motorId</th>\n",
       "      <td>adi#0c7dbd85-a5e7-4e6a-b37c-a49e1adaca45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempe</th>\n",
       "      <td>[25.875, 25.875]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempm</th>\n",
       "      <td>[25.9375, 25.9375]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tenantId</th>\n",
       "      <td>adi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <td>1611756186321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <td>60814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsr</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vbat</th>\n",
       "      <td>[3.14817659, 3.130985182, 3.122389478, 3.14817...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibx</th>\n",
       "      <td>[-0.15531, -0.48733000000000004, 0.84075000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibz</th>\n",
       "      <td>[-2.41427, -0.42215, -1.08619, -0.256140000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0\n",
       "dataType                                      completeSample\n",
       "deviceId                                           00000781O\n",
       "flux       [31.3639, 23.8975, 35.5119, 27.0085, 16.6385, ...\n",
       "motorId             adi#0c7dbd85-a5e7-4e6a-b37c-a49e1adaca45\n",
       "tempe                                       [25.875, 25.875]\n",
       "tempm                                     [25.9375, 25.9375]\n",
       "tenantId                                                 adi\n",
       "timestamp                                      1611756186321\n",
       "ts                                                     60814\n",
       "tsr                                                    False\n",
       "vbat       [3.14817659, 3.130985182, 3.122389478, 3.14817...\n",
       "vibx       [-0.15531, -0.48733000000000004, 0.84075000000...\n",
       "vibz       [-2.41427, -0.42215, -1.08619, -0.256140000000..."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k, v = s.head()\n",
    "print(f\"{k=}\")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad2dcd4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:19:33.389611Z",
     "start_time": "2023-02-08T13:19:32.954311Z"
    }
   },
   "outputs": [],
   "source": [
    "from hubcap import GitHubReader, Github\n",
    "\n",
    "\n",
    "t = GitHubReader('i2mint')\n",
    "tt = list(t);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83a95597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:40:42.966502Z",
     "start_time": "2023-02-08T13:40:42.942188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[aiofiledol](https://www.github.com/i2mint/aiofiledol)\n",
      "    * [arangodol](https://www.github.com/i2mint/arangodol)\n",
      "    * [couchdol](https://www.github.com/i2mint/couchdol)\n",
      "    * [dol](https://www.github.com/i2mint/dol)\n",
      "    * [dropboxdol](https://www.github.com/i2mint/dropboxdol)\n",
      "    * [dynamodol](https://www.github.com/i2mint/dynamodol)\n",
      "    * [ftpdol](https://www.github.com/i2mint/ftpdol)\n",
      "    * [mongodol](https://www.github.com/i2mint/mongodol)\n",
      "    * [odbcdol](https://www.github.com/i2mint/odbcdol)\n",
      "    * [pydrivedol](https://www.github.com/i2mint/pydrivedol)\n",
      "    * [redisdol](https://www.github.com/i2mint/redisdol)\n",
      "    * [s3dol](https://www.github.com/i2mint/s3dol)\n",
      "    * [sqldol](https://www.github.com/i2mint/sqldol)\n",
      "    * [sshdol](https://www.github.com/i2mint/sshdol)\n"
     ]
    }
   ],
   "source": [
    "w = filter(lambda x: x.endswith('dol'), tt)\n",
    "w = map(lambda x: f\"[{x}](https://www.github.com/i2mint/{x})\", w)\n",
    "print(*w, sep='\\n    * ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1b1aad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:55:21.212751Z",
     "start_time": "2023-02-08T13:55:20.755353Z"
    }
   },
   "outputs": [],
   "source": [
    "from redisdol import RedisBytesPersister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39bb6a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:58:03.557941Z",
     "start_time": "2023-02-08T13:58:03.531262Z"
    }
   },
   "outputs": [],
   "source": [
    "s = RedisBytesPersister()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3d5c7c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:58:05.964609Z",
     "start_time": "2023-02-08T13:58:05.928849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec242408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:00:04.123079Z",
     "start_time": "2023-02-09T09:00:04.114326Z"
    }
   },
   "outputs": [],
   "source": [
    "t = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bbf847b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:00:12.277889Z",
     "start_time": "2023-02-09T09:00:12.255548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t += [5, 6]\n",
    "# t.extend([5, 6])\n",
    "t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63a6fb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:00:19.100109Z",
     "start_time": "2023-02-09T09:00:19.092488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 8]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.append(8)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721bb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[k].append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20e6fd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:01:48.504289Z",
     "start_time": "2023-02-09T09:01:48.227157Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pwd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Files\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpwd\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pwd' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91119a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963eab94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817918b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e807f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79810cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14dde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0530775d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
