{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f0aa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T14:01:45.617689Z",
     "start_time": "2023-02-03T14:01:45.591409Z"
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd12691",
   "metadata": {},
   "source": [
    "# Combined datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05bef4",
   "metadata": {},
   "source": [
    "<table style=\"border: 0; border-collapse: collapse; border-spacing: 0;\">\n",
    "  <tr>\n",
    "    <td style=\"width:50%; border: none;\">\n",
    "    Problem: Data analysis frequently necessitates the manipulation of datasets scattered across various tables, such as those found in pandas DataFrames or SQL databases. The integration and examination of this distributed data mandate the execution of table joins via shared fields. Although this operation is not inherently difficult, it can become cumbersome when the datasets are extensive or when numerous tables are involved.\n",
    "    <p><p>\n",
    "    See more in discussion: <a href=\"https://github.com/i2mint/tabled/discussions/3\">https://github.com/i2mint/tabled/discussions/3</a>\n",
    "    </td>\n",
    "    <td style=\"border: none;\">\n",
    "      <img src=\"https://github.com/i2mint/tabled/assets/1906276/37dd2e30-3792-4d0b-94a5-c48e83986061\" width=320 alt=\"Venn diagram of fields\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69264267",
   "metadata": {},
   "source": [
    "## Making get_table_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29df2d",
   "metadata": {},
   "source": [
    "We want to make this:\n",
    "\n",
    "```python\n",
    "def get_table_join(tables, fields):\n",
    "    \"\"\"\n",
    "    Get table with requested `fields`, computed by joining relevant tables \n",
    "    of `tables`.\n",
    "    \"\"\"\n",
    "    resolution_sequence = join_resolution(tables, fields)\n",
    "    return compute_join_resolution(resolution_sequence, tables)\n",
    "\n",
    "# where...\n",
    "\n",
    "def join_resolution(field_sets: dict, fields_to_cover: Iterable) -> list:\n",
    "    \"\"\"\n",
    "    Returns the list of join operations that, when carried out, \n",
    "    cover the given fields.\n",
    "    \n",
    "    :param field_sets: A mapping of table names to sets of their fields.\n",
    "    :param fields: The fields to cover.\n",
    "    \"\"\"\n",
    "\n",
    "def compute_join_resolution(\n",
    "        resolution_sequence: Iterable, tables: Mapping[str, pd.DataFrame]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carries `resolution_sequence` join operations out with tables taken \n",
    "    from `tables`.\n",
    "    \n",
    "    :param resolution_sequence: An iterable of join operations to carry out. \n",
    "        Each join operation is either a table name (str) or a JoinWith object.\n",
    "        If it's a JoinWith object, it's assumed that the table has already \n",
    "        been joined and the fields to remove are in the `remove` \n",
    "        attribute of the object.\n",
    "    :param tables: A mapping of table names to tables (pd.DataFrame)\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "699a8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable, Mapping\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tables = {\n",
    "    \"A\": pd.DataFrame({'b': [1, 2, 3, 33], 'c': [4, 5, 6, 66]}),\n",
    "    \"B\": pd.DataFrame(\n",
    "        {'b': [1, 2, 3], 'a': [4, 5, 6], 'd': [7, 8, 9], \n",
    "         'e': [10, 11, 12], 'f': [13, 14, 15]}\n",
    "    ),\n",
    "    \"C\": pd.DataFrame({'f': [13, 14, 15], 'g': [4, 5, 6]}),\n",
    "    \"D\": pd.DataFrame(\n",
    "        {'d': [7, 8, 77], 'e': [10, 11, 77], 'h': [7, 8, 9], 'i': [1, 2, 3]}\n",
    "    ),\n",
    "    \"E\": pd.DataFrame({'i': [1, 2, 3], 'j': [4, 5, 6]})\n",
    "}\n",
    "\n",
    "field_sets = {table_id: set(df.columns) for table_id, df in tables.items()}\n",
    "assert field_sets == {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class JoinWith:\n",
    "    table_key: str\n",
    "    remove: list = None\n",
    "\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "expected_join_resolution = [\n",
    "    'B',\n",
    "    JoinWith('C', remove=['a', 'f']),\n",
    "    JoinWith('D', remove=['d', 'e', 'h']),\n",
    "    JoinWith('E', remove=['i'])\n",
    "]\n",
    "expected_result = pd.DataFrame({\n",
    "    'b': [1, 2],\n",
    "    'g': [4, 5],\n",
    "    'j': [4, 5]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2d934",
   "metadata": {},
   "source": [
    "The tests would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b3b95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_join_resolution(\n",
    "        join_resolution: Callable,\n",
    "        *,\n",
    "        field_sets: dict = field_sets, \n",
    "        fields_to_cover: Iterable = fields_to_cover,\n",
    "        expected_join_resolution: list = expected_join_resolution,\n",
    "    ):\n",
    "    assert join_resolution(field_sets, fields_to_cover) == expected_join_resolution\n",
    "\n",
    "def test_compute_join_resolution(\n",
    "    compute_join_resolution: Callable, \n",
    "    *,\n",
    "    resolution_sequence: Iterable = expected_join_resolution,\n",
    "    tables: Mapping[str, pd.DataFrame] = tables,\n",
    "    expected_result: pd.DataFrame = expected_result,   \n",
    "):\n",
    "    result = compute_join_resolution(resolution_sequence, tables)\n",
    "    assert tables_are_equal(result, expected_result)\n",
    "\n",
    "\n",
    "def tables_are_equal(\n",
    "        t1, t2, *, ignore_index=True, sort_index=True, sort_columns=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Compare two tables for equality, ignoring (by default) index and column order.\n",
    "    \"\"\"\n",
    "    if ignore_index:\n",
    "        t1 = t1.reset_index(drop=True)\n",
    "        t2 = t2.reset_index(drop=True)\n",
    "    elif sort_index:\n",
    "        t1 = t1.sort_index(axis=1)\n",
    "        t2 = t2.sort_index(axis=1)\n",
    "    if sort_columns:\n",
    "        t1 = t1.sort_index(axis=0)\n",
    "        t2 = t2.sort_index(axis=0)\n",
    "    return t1.equals(t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0112fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_join_op(obj):\n",
    "    if not isinstance(obj, JoinWith):\n",
    "        return JoinWith(obj)\n",
    "    return obj\n",
    "        \n",
    "\n",
    "def compute_join_resolution(\n",
    "        resolution_sequence: Iterable, tables: Mapping[str, pd.DataFrame]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carries `resolution_sequence` join operations out with tables taken from `tables`.\n",
    "    \n",
    "    :param resolution_sequence: An iterable of join operations to carry out. \n",
    "        Each join operation is either a table name (str) or a JoinWith object.\n",
    "        If it's a JoinWith object, it's assumed that the table has already been joined\n",
    "        and the fields to remove are in the `remove` attribute of the object.\n",
    "    :param tables: A mapping of table names to tables (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    join_ops = map(ensure_join_op, resolution_sequence)\n",
    "    table_key = next(join_ops).table_key\n",
    "    joined = tables[table_key]\n",
    "    for join_op in join_ops:\n",
    "        table = tables[join_op.table_key]\n",
    "        joined = joined.merge(table, how='inner')\n",
    "        if join_op.remove:\n",
    "            remove_cols = set(join_op.remove) & set(joined.columns)\n",
    "            joined = joined.drop(columns=remove_cols)\n",
    "    return joined\n",
    "\n",
    "test_compute_join_resolution(compute_join_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_sets == {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "\n",
    "def ensure_join_op(obj):\n",
    "    if not isinstance(obj, JoinWith):\n",
    "        return JoinWith(obj)\n",
    "    return obj\n",
    "        \n",
    "\n",
    "def compute_join_resolution(\n",
    "        resolution_sequence: Iterable, tables: Mapping[str, pd.DataFrame]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carries `resolution_sequence` join operations out with tables taken from `tables`.\n",
    "    \n",
    "    :param resolution_sequence: An iterable of join operations to carry out. \n",
    "        Each join operation is either a table name (str) or a JoinWith object.\n",
    "        If it's a JoinWith object, it's assumed that the table has already been joined\n",
    "        and the fields to remove are in the `remove` attribute of the object.\n",
    "    :param tables: A mapping of table names to tables (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    join_ops = map(ensure_join_op, resolution_sequence)\n",
    "    table_key = next(join_ops).table_key\n",
    "    joined = tables[table_key]\n",
    "    for join_op in join_ops:\n",
    "        table = tables[join_op.table_key]\n",
    "        joined = joined.merge(table, how='inner')\n",
    "        if join_op.remove:\n",
    "            remove_cols = set(join_op.remove) & set(joined.columns)\n",
    "            joined = joined.drop(columns=remove_cols)\n",
    "    return joined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ced6884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', JoinWith(table_key='C', remove=[]), JoinWith(table_key='E', remove=[])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_join_resolution(\n",
    "        join_resolution: Callable,\n",
    "        *,\n",
    "        field_sets: dict = field_sets, \n",
    "        fields_to_cover: Iterable = fields_to_cover,\n",
    "        expected_join_resolution: list = expected_join_resolution,\n",
    "    ):\n",
    "    assert join_resolution(field_sets, fields_to_cover) == expected_join_resolution\n",
    "\n",
    "\n",
    "# Given test data and expected results\n",
    "field_sets = {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "\n",
    "\n",
    "def join_resolution(field_sets: dict, fields_to_cover: Iterable[str]) -> list:\n",
    "    fields_to_cover = set(fields_to_cover)\n",
    "    field_sets = field_sets.copy()\n",
    "    resolution_sequence = []\n",
    "    # Initialize covered fields with an empty set\n",
    "    covered_fields = set()\n",
    "\n",
    "    # Loop until we've covered all fields\n",
    "    while not fields_to_cover.issubset(covered_fields):\n",
    "        # Find the table that has the most uncovered fields to cover\n",
    "        next_table = max(\n",
    "            field_sets.keys(),\n",
    "            key=lambda t: len(fields_to_cover.intersection(field_sets[t])) - len(covered_fields.intersection(field_sets[t])),\n",
    "            default=None\n",
    "        )\n",
    "\n",
    "        if next_table is None:\n",
    "            # If we can't find a next table, it means we can't cover the fields with the given field sets\n",
    "            raise ValueError(\"Cannot cover all fields with the given field sets.\")\n",
    "\n",
    "        # Determine which fields from the new table we will need to remove after the join\n",
    "        fields_to_remove = (covered_fields & field_sets[next_table]) - fields_to_cover\n",
    "\n",
    "        # If this is not the first table, we create a JoinWith instance, otherwise, just add the table name\n",
    "        if resolution_sequence:\n",
    "            resolution_sequence.append(JoinWith(table_key=next_table, remove=list(fields_to_remove)))\n",
    "        else:\n",
    "            resolution_sequence.append(next_table)\n",
    "\n",
    "        # Update covered fields\n",
    "        covered_fields.update(field_sets[next_table])\n",
    "\n",
    "        # Remove the used table from field_sets to prevent reusing it\n",
    "        del field_sets[next_table]\n",
    "\n",
    "    return resolution_sequence\n",
    "\n",
    "# test_join_resolution(join_resolution)\n",
    "join_resolution(field_sets, fields_to_cover)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8480d4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'B')}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given test data and expected results\n",
    "field_sets = {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "\n",
    "def test_minimum_covering_intersections(\n",
    "        minimum_covering_intersections: Callable,\n",
    "        *,\n",
    "        field_sets: dict = field_sets, \n",
    "        fields_to_cover: Iterable = fields_to_cover,\n",
    "        expected_result: set = {('B', 'C'), ('B', 'D'), ('B', 'E')}\n",
    "    ):\n",
    "    assert minimum_covering_intersections(field_sets, fields_to_cover) == expected_result\n",
    "\n",
    "\n",
    "def minimum_covering_intersections(field_sets, fields_to_cover):\n",
    "    \"\"\"Returns a set of set pairs that connect the elements of `fields_to_cover`.\n",
    "    \n",
    "    :param field_sets: A mapping of sets of field names\n",
    "    :param fields_to_cover: A set of field names to cover\n",
    "    :return: A set of set pairs that connect the elements of `fields_to_cover`\n",
    "    \n",
    "    The returned pairs are `(key_1, key_2)` pairs of keys of `field_sets`\n",
    "    such that field_sets[key_1] & field_sets[key_2] is not empty.\n",
    "    The union of all the sets in the pairs should cover `fields_to_cover`.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "def minimum_covering_intersections(field_sets, fields_to_cover):\n",
    "    fields_to_cover = set(fields_to_cover)  # Ensure we have a set to work with\n",
    "    covering_pairs = set()\n",
    "    covered_fields = set()\n",
    "\n",
    "    # Go through all unique pairs of keys from field_sets\n",
    "    for key1, key2 in combinations(field_sets.keys(), 2):\n",
    "        intersection = field_sets[key1] & field_sets[key2]\n",
    "        \n",
    "        # Check if this pair covers any new fields that we need to cover\n",
    "        newly_covered = intersection & fields_to_cover - covered_fields\n",
    "        if newly_covered:\n",
    "            covering_pairs.add((key1, key2))\n",
    "            covered_fields.update(newly_covered)\n",
    "            \n",
    "        # If we've covered all fields, we can stop early\n",
    "        if covered_fields == fields_to_cover:\n",
    "            break\n",
    "\n",
    "    return covering_pairs\n",
    "\n",
    "minimum_covering_intersections(field_sets, fields_to_cover)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2b34823d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {'b', 'c'},\n",
       " 'B': {'a', 'b', 'd', 'e', 'f'},\n",
       " 'C': {'f', 'g'},\n",
       " 'D': {'d', 'e', 'h', 'i'},\n",
       " 'E': {'i', 'j'}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e3256c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, KT, Literal\n",
    "\n",
    "def collection_adjacencies(\n",
    "        sets: Dict[KT, set], \n",
    "        edge_labels: Literal['elements', 'size', False] = False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    A mapping of vertices to their adjacent vertices, along with their edge weights.\n",
    "    Here, vertices are the values of sets, \n",
    "    and there is an edge between two vertices if the sets intersect.\n",
    "    The weight of the edge is the size of the intersection.\n",
    "\n",
    "    :param sets: A mapping of keys to sets of elements\n",
    "    :param edge_labels: If 'elements', the edge labels are the elements of the intersection.\n",
    "        If 'size', the edge labels are the size of the intersection.\n",
    "        If False, there are no edge labels.\n",
    "\n",
    "    >>> sets = {\n",
    "    ...     'A': {'b', 'c'},\n",
    "    ...     'B': {'a', 'b', 'd', 'e', 'f'},\n",
    "    ...     'C': {'f', 'g'},\n",
    "    ...     'D': {'d', 'e', 'h', 'i'},\n",
    "    ...     'E': {'i', 'j'}\n",
    "    ... }\n",
    "    >>> assert collection_adjacencies(sets) == {\n",
    "    ...     'A': {'B'}, 'B': {'A', 'C', 'D'}, 'C': {'B'}, 'D': {'B', 'E'}, 'E': {'D'}\n",
    "    ... }\n",
    "    >>> assert collection_adjacencies(sets, edge_labels='elements') == {\n",
    "    ...     'A': {'B': {'b'}},\n",
    "    ...     'B': {'A': {'b'}, 'C': {'f'}, 'D': {'d', 'e'}},\n",
    "    ...     'C': {'B': {'f'}},\n",
    "    ...     'D': {'B': {'d', 'e'}, 'E': {'i'}},\n",
    "    ...     'E': {'D': {'i'}}\n",
    "    ... }\n",
    "    >>> assert collection_adjacencies(sets, edge_labels='size') == {\n",
    "    ...     'A': {'B': 1},\n",
    "    ...     'B': {'A': 1, 'C': 1, 'D': 2},\n",
    "    ...     'C': {'B': 1},\n",
    "    ...     'D': {'B': 2, 'E': 1},\n",
    "    ...     'E': {'D': 1}\n",
    "    ... }\n",
    "\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    from itertools import combinations\n",
    "\n",
    "    graph = defaultdict(dict)\n",
    "    for key1, key2 in combinations(sets.keys(), 2):\n",
    "        intersection = sets[key1] & sets[key2]\n",
    "        if intersection:\n",
    "            # graph.setdefault(key1, {}).setdefault(key2, 0)\n",
    "            # graph.setdefault(key2, {}).setdefault(key1, 0)\n",
    "            graph[key1][key2] = intersection\n",
    "            graph[key2][key1] = intersection\n",
    "    if isinstance(edge_labels, str):\n",
    "        if edge_labels == 'elements':\n",
    "            return dict(graph)\n",
    "        elif edge_labels == 'size':\n",
    "            return {k: {kk: len(vv) for kk, vv in v.items()} for k, v in graph.items()}\n",
    "    elif edge_labels is False:\n",
    "        return {k: set(v) for k, v in graph.items()}\n",
    "    \n",
    "    raise ValueError(f\"Invalid value for edge_labels: {edge_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e32a1d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " JoinWith(table_key='C', remove=['f', 'c', 'b']),\n",
       " JoinWith(table_key='E', remove=['g', 'i', 'f', 'b', 'c'])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterable, Mapping\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "# Define the JoinWith dataclass\n",
    "@dataclass\n",
    "class JoinWith:\n",
    "    table_key: str\n",
    "    remove: list = None\n",
    "\n",
    "# Define the function to find the join resolution\n",
    "def join_resolution(field_sets: dict, fields_to_cover: Iterable[str]) -> list:\n",
    "    fields_to_cover = set(fields_to_cover)  # Set of fields we need to cover\n",
    "    join_sequence = []  # The output sequence of joins\n",
    "    covered_fields = set()  # Fields covered by the join sequence so far\n",
    "\n",
    "    # While there are fields left to cover, iterate\n",
    "    while fields_to_cover:\n",
    "        # Find the table that covers the most fields which are not yet covered\n",
    "        next_table, next_fields = max(\n",
    "            field_sets.items(),\n",
    "            key=lambda item: len(fields_to_cover.intersection(item[1])) - len(covered_fields.intersection(item[1])),\n",
    "            default=(None, None)\n",
    "        )\n",
    "\n",
    "        # If no table can cover any more fields, break the loop\n",
    "        if not next_table:\n",
    "            break\n",
    "\n",
    "        # Determine which fields we will need to remove after joining this table\n",
    "        fields_to_remove = (covered_fields | next_fields) - fields_to_cover\n",
    "\n",
    "        # Add the next table to the join sequence\n",
    "        if join_sequence:\n",
    "            join_sequence.append(JoinWith(next_table, remove=list(fields_to_remove)))\n",
    "        else:  # If this is the first table, we don't need to remove any fields\n",
    "            join_sequence.append(next_table)\n",
    "\n",
    "        # Update the sets of covered fields and the remaining fields to cover\n",
    "        covered_fields.update(next_fields)\n",
    "        fields_to_cover -= next_fields\n",
    "\n",
    "        # Remove the chosen table from field_sets to avoid re-selecting it\n",
    "        del field_sets[next_table]\n",
    "\n",
    "    # Ensure that the last operation does not include a removal of fields from the last table joined\n",
    "    if join_sequence and isinstance(join_sequence[-1], JoinWith):\n",
    "        join_sequence[-1].remove = [f for f in join_sequence[-1].remove if f in covered_fields - fields_to_cover]\n",
    "\n",
    "    return join_sequence\n",
    "\n",
    "# Given test data and expected results\n",
    "field_sets = {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "\n",
    "# Expected output\n",
    "expected_join_resolution = [\n",
    "    'B',\n",
    "    JoinWith('C', remove=['a', 'f']),\n",
    "    JoinWith('D', remove=['d', 'e', 'h']),\n",
    "    JoinWith('E', remove=['i'])\n",
    "]\n",
    "\n",
    "# Run the function with the test data\n",
    "join_resolution_output = join_resolution(field_sets.copy(), fields_to_cover)\n",
    "\n",
    "# Display the output\n",
    "join_resolution_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2332b993",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ['A',\n",
    " JoinWith(table_key='B', remove=['b']),\n",
    " JoinWith(table_key='C', remove=['b']),\n",
    " JoinWith(table_key='E', remove=['g', 'b'])]\n",
    "\n",
    "# compute_join_resolution(t, tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9279175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert field_sets == {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "assert fields_to_cover == ['b', 'g', 'j']\n",
    "assert expected_join_resolution == [\n",
    "    'B',\n",
    "    JoinWith('C', remove=['a', 'f']),\n",
    "    JoinWith('D', remove=['d', 'e', 'h']),\n",
    "    JoinWith('E', remove=['i'])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017edbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9ce189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tables_are_equal(\n",
    "        t1, t2, *, ignore_index=True, sort_index=True, sort_columns=True\n",
    "    ):\n",
    "    if ignore_index:\n",
    "        t1 = t1.reset_index(drop=True)\n",
    "        t2 = t2.reset_index(drop=True)\n",
    "    elif sort_index:\n",
    "        t1 = t1.sort_index(axis=1)\n",
    "        t2 = t2.sort_index(axis=1)\n",
    "    if sort_columns:\n",
    "        t1 = t1.sort_index(axis=0)\n",
    "        t2 = t2.sort_index(axis=0)\n",
    "    return t1.equals(t2)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tables = {\n",
    "    \"A\": pd.DataFrame({'b': [1, 2, 3, 33], 'c': [4, 5, 6, 66]}),\n",
    "    \"B\": pd.DataFrame({'b': [1, 2, 3], 'a': [4, 5, 6], 'd': [7, 8, 9], 'e': [10, 11, 12], 'f': [13, 14, 15]}),\n",
    "    \"C\": pd.DataFrame({'f': [13, 14, 15], 'g': [4, 5, 6]}),\n",
    "    \"D\": pd.DataFrame({'d': [7, 8, 77], 'e': [10, 11, 77], 'h': [7, 8, 9], 'i': [1, 2, 3]}),\n",
    "    \"E\": pd.DataFrame({'i': [1, 2, 3], 'j': [4, 5, 6]})\n",
    "}\n",
    "\n",
    "field_sets = {table_id: set(df.columns) for table_id, df in tables.items()}\n",
    "assert field_sets == {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "\n",
    "from typing import Callable, Iterable, Mapping\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class JoinWith:\n",
    "    table_key: str\n",
    "    remove: list = None\n",
    "\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "expected_join_resolution = [\n",
    "    'B',\n",
    "    JoinWith('C', remove=['a', 'f']),\n",
    "    JoinWith('D', remove=['d', 'e', 'h']),\n",
    "    JoinWith('E', remove=['i'])\n",
    "]\n",
    "expected_result = pd.DataFrame({\n",
    "    'b': [1, 2],\n",
    "    'g': [4, 5],\n",
    "    'j': [4, 5]\n",
    "})\n",
    "\n",
    "\n",
    "    \n",
    "def covers(field_sets: dict, fields_to_cover: Iterable) -> list:\n",
    "    \"\"\"\n",
    "    Returns the list of table names that cover the given fields.\n",
    "    \n",
    "    :param field_sets: A mapping of table names to the fields (columns) they contain.\n",
    "    :param fields_to_cover: The fields to cover.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        table_name for table_name, cols in field_sets.items() \n",
    "        if set(fields_to_cover) <= cols\n",
    "    ]\n",
    "\n",
    "\n",
    "def test_covers(\n",
    "        *,\n",
    "        field_sets: dict = field_sets, \n",
    "        fields_to_cover: Iterable = ['b', 'g', 'j'],\n",
    "        expected_table_ids: list = ['B', 'C', 'D', 'E'],\n",
    "        covers: Callable = covers,\n",
    "    ):\n",
    "    assert sorted(covers(field_sets, fields_to_cover)) == sorted(expected_table_ids)\n",
    "\n",
    "\n",
    "def join_resolution(field_sets: dict, fields_to_cover: Iterable) -> list:\n",
    "    \"\"\"\n",
    "    Returns the list of join operations that, when carried out, cover the given fields.\n",
    "    \n",
    "    :param field_sets: A mapping of table names to sets of their fields.\n",
    "    :param fields: The fields to cover.\n",
    "    \"\"\"\n",
    "    resolution_sequence = []\n",
    "    while fields_to_cover:\n",
    "        table_ids = covers(field_sets, fields_to_cover)\n",
    "        if not table_ids:\n",
    "            raise ValueError(f\"Can't cover fields {fields_to_cover} with the given tables.\")\n",
    "        table_id = table_ids[0]\n",
    "        fields_to_cover = set(fields_to_cover) - field_sets[table_id]\n",
    "        resolution_sequence.append(table_id)\n",
    "    return resolution_sequence\n",
    "\n",
    "\n",
    "def test_join_resolution(\n",
    "        *,\n",
    "        field_sets: dict, \n",
    "        fields_to_cover: Iterable,\n",
    "        expected_join_resolution: list,\n",
    "        join_resolution: Callable,\n",
    "    ):\n",
    "    assert join_resolution(field_sets, fields_to_cover) == expected_join_resolution\n",
    "\n",
    "\n",
    "def ensure_join_op(obj):\n",
    "    if not isinstance(obj, JoinWith):\n",
    "        return JoinWith(obj)\n",
    "    return obj\n",
    "        \n",
    "\n",
    "def compute_join_resolution(\n",
    "        resolution_sequence: Iterable, tables: Mapping[str, pd.DataFrame]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carries `resolution_sequence` join operations out with tables taken from `tables`.\n",
    "    \n",
    "    :param resolution_sequence: An iterable of join operations to carry out. \n",
    "        Each join operation is either a table name (str) or a JoinWith object.\n",
    "        If it's a JoinWith object, it's assumed that the table has already been joined\n",
    "        and the fields to remove are in the `remove` attribute of the object.\n",
    "    :param tables: A mapping of table names to tables (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    join_ops = map(ensure_join_op, resolution_sequence)\n",
    "    table_key = next(join_ops).table_key\n",
    "    joined = tables[table_key]\n",
    "    for join_op in join_ops:\n",
    "        table = tables[join_op.table_key]\n",
    "        joined = joined.merge(table, how='inner')\n",
    "        if join_op.remove:\n",
    "            remove_cols = set(join_op.remove) & set(joined.columns)\n",
    "            joined = joined.drop(columns=remove_cols)\n",
    "    return joined\n",
    "\n",
    "def test_compute_join_resolution(\n",
    "    *,\n",
    "    resolution_sequence: Iterable = expected_join_resolution,\n",
    "    tables: Mapping[str, pd.DataFrame] = tables,\n",
    "    expected_result: pd.DataFrame = expected_result,\n",
    "    compute_join_resolution: Callable = compute_join_resolution,    \n",
    "):\n",
    "    resolution_sequence = expected_join_resolution\n",
    "    result = compute_join_resolution(resolution_sequence, tables)\n",
    "    assert tables_are_equal(result, expected_result)\n",
    "\n",
    "test_compute_join_resolution()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c5715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b8c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f1848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc323b3e",
   "metadata": {},
   "source": [
    "# Proposal: tiny light framework for routing with mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0acf755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T12:07:39.913553Z",
     "start_time": "2023-02-06T12:07:39.875060Z"
    }
   },
   "outputs": [],
   "source": [
    "from i2.routing_forest import KeyFuncMapping\n",
    "from tabled import dflt_ext_mapping, get_ext\n",
    "\n",
    "from dol import Files, FilesOfZip, Pipe, wrap_kvs\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import io\n",
    "\n",
    "ext = KeyFuncMapping(\n",
    "    dict(\n",
    "        dflt_ext_mapping, \n",
    "        json=Pipe(io.BytesIO, partial(pd.read_json, orient='index'))\n",
    "    ),\n",
    "    get_ext,\n",
    ")\n",
    "\n",
    "table_trans = wrap_kvs(postget=lambda k, v: ext[k](v))\n",
    "\n",
    "TableFiles = table_trans(Files)\n",
    "TableZipFiles = table_trans(FilesOfZip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cacd8921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T12:07:44.075672Z",
     "start_time": "2023-02-06T12:07:43.973256Z"
    }
   },
   "outputs": [],
   "source": [
    "s = TableZipFiles('/Users/thorwhalen/Dropbox/_odata/sound/induction_motor_data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d016bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T12:07:45.572475Z",
     "start_time": "2023-02-06T12:07:45.547063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3909"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3d28ae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T14:50:43.104488Z",
     "start_time": "2023-02-03T14:50:43.053751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k='induction_motor_data/Bearing/Bearing_1250rpm/2021_01_27_15_03_06.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dataType</th>\n",
       "      <td>completeSample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deviceId</th>\n",
       "      <td>00000781O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flux</th>\n",
       "      <td>[31.3639, 23.8975, 35.5119, 27.0085, 16.6385, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motorId</th>\n",
       "      <td>adi#0c7dbd85-a5e7-4e6a-b37c-a49e1adaca45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempe</th>\n",
       "      <td>[25.875, 25.875]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempm</th>\n",
       "      <td>[25.9375, 25.9375]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tenantId</th>\n",
       "      <td>adi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <td>1611756186321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <td>60814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsr</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vbat</th>\n",
       "      <td>[3.14817659, 3.130985182, 3.122389478, 3.14817...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibx</th>\n",
       "      <td>[-0.15531, -0.48733000000000004, 0.84075000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibz</th>\n",
       "      <td>[-2.41427, -0.42215, -1.08619, -0.256140000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0\n",
       "dataType                                      completeSample\n",
       "deviceId                                           00000781O\n",
       "flux       [31.3639, 23.8975, 35.5119, 27.0085, 16.6385, ...\n",
       "motorId             adi#0c7dbd85-a5e7-4e6a-b37c-a49e1adaca45\n",
       "tempe                                       [25.875, 25.875]\n",
       "tempm                                     [25.9375, 25.9375]\n",
       "tenantId                                                 adi\n",
       "timestamp                                      1611756186321\n",
       "ts                                                     60814\n",
       "tsr                                                    False\n",
       "vbat       [3.14817659, 3.130985182, 3.122389478, 3.14817...\n",
       "vibx       [-0.15531, -0.48733000000000004, 0.84075000000...\n",
       "vibz       [-2.41427, -0.42215, -1.08619, -0.256140000000..."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k, v = s.head()\n",
    "print(f\"{k=}\")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad2dcd4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:19:33.389611Z",
     "start_time": "2023-02-08T13:19:32.954311Z"
    }
   },
   "outputs": [],
   "source": [
    "from hubcap import GitHubReader, Github\n",
    "\n",
    "\n",
    "t = GitHubReader('i2mint')\n",
    "tt = list(t);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83a95597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:40:42.966502Z",
     "start_time": "2023-02-08T13:40:42.942188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[aiofiledol](https://www.github.com/i2mint/aiofiledol)\n",
      "    * [arangodol](https://www.github.com/i2mint/arangodol)\n",
      "    * [couchdol](https://www.github.com/i2mint/couchdol)\n",
      "    * [dol](https://www.github.com/i2mint/dol)\n",
      "    * [dropboxdol](https://www.github.com/i2mint/dropboxdol)\n",
      "    * [dynamodol](https://www.github.com/i2mint/dynamodol)\n",
      "    * [ftpdol](https://www.github.com/i2mint/ftpdol)\n",
      "    * [mongodol](https://www.github.com/i2mint/mongodol)\n",
      "    * [odbcdol](https://www.github.com/i2mint/odbcdol)\n",
      "    * [pydrivedol](https://www.github.com/i2mint/pydrivedol)\n",
      "    * [redisdol](https://www.github.com/i2mint/redisdol)\n",
      "    * [s3dol](https://www.github.com/i2mint/s3dol)\n",
      "    * [sqldol](https://www.github.com/i2mint/sqldol)\n",
      "    * [sshdol](https://www.github.com/i2mint/sshdol)\n"
     ]
    }
   ],
   "source": [
    "w = filter(lambda x: x.endswith('dol'), tt)\n",
    "w = map(lambda x: f\"[{x}](https://www.github.com/i2mint/{x})\", w)\n",
    "print(*w, sep='\\n    * ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1b1aad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:55:21.212751Z",
     "start_time": "2023-02-08T13:55:20.755353Z"
    }
   },
   "outputs": [],
   "source": [
    "from redisdol import RedisBytesPersister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39bb6a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:58:03.557941Z",
     "start_time": "2023-02-08T13:58:03.531262Z"
    }
   },
   "outputs": [],
   "source": [
    "s = RedisBytesPersister()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3d5c7c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:58:05.964609Z",
     "start_time": "2023-02-08T13:58:05.928849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec242408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:00:04.123079Z",
     "start_time": "2023-02-09T09:00:04.114326Z"
    }
   },
   "outputs": [],
   "source": [
    "t = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bbf847b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:00:12.277889Z",
     "start_time": "2023-02-09T09:00:12.255548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t += [5, 6]\n",
    "# t.extend([5, 6])\n",
    "t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63a6fb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:00:19.100109Z",
     "start_time": "2023-02-09T09:00:19.092488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 8]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.append(8)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721bb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[k].append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20e6fd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:01:48.504289Z",
     "start_time": "2023-02-09T09:01:48.227157Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pwd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Files\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpwd\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pwd' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91119a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963eab94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817918b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e807f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79810cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14dde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0530775d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
