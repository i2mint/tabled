{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f0aa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T14:01:45.617689Z",
     "start_time": "2023-02-03T14:01:45.591409Z"
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd12691",
   "metadata": {},
   "source": [
    "# Combined datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05bef4",
   "metadata": {},
   "source": [
    "<table style=\"border: 0; border-collapse: collapse; border-spacing: 0;\">\n",
    "  <tr>\n",
    "    <td style=\"width:50%; border: none;\">\n",
    "    Problem: Data analysis frequently necessitates the manipulation of datasets scattered across various tables, such as those found in pandas DataFrames or SQL databases. The integration and examination of this distributed data mandate the execution of table joins via shared fields. Although this operation is not inherently difficult, it can become cumbersome when the datasets are extensive or when numerous tables are involved.\n",
    "    <p><p>\n",
    "    See more in discussion: <a href=\"https://github.com/i2mint/tabled/discussions/3\">https://github.com/i2mint/tabled/discussions/3</a>\n",
    "    </td>\n",
    "    <td style=\"border: none;\">\n",
    "      <img src=\"https://github.com/i2mint/tabled/assets/1906276/37dd2e30-3792-4d0b-94a5-c48e83986061\" width=320 alt=\"Venn diagram of fields\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69264267",
   "metadata": {},
   "source": [
    "## Making get_table_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29df2d",
   "metadata": {},
   "source": [
    "We want to make this:\n",
    "\n",
    "```python\n",
    "def get_table_join(tables, fields):\n",
    "    \"\"\"\n",
    "    Get table with requested `fields`, computed by joining relevant tables \n",
    "    of `tables`.\n",
    "    \"\"\"\n",
    "    resolution_sequence = join_resolution(tables, fields)\n",
    "    return compute_join_resolution(resolution_sequence, tables)\n",
    "\n",
    "# where...\n",
    "\n",
    "def join_resolution(field_sets: dict, fields_to_cover: Iterable) -> list:\n",
    "    \"\"\"\n",
    "    Returns the list of join operations that, when carried out, \n",
    "    cover the given fields.\n",
    "    \n",
    "    :param field_sets: A mapping of table names to sets of their fields.\n",
    "    :param fields: The fields to cover.\n",
    "    \"\"\"\n",
    "\n",
    "def compute_join_resolution(\n",
    "        resolution_sequence: Iterable, tables: Mapping[str, pd.DataFrame]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carries `resolution_sequence` join operations out with tables taken \n",
    "    from `tables`.\n",
    "    \n",
    "    :param resolution_sequence: An iterable of join operations to carry out. \n",
    "        Each join operation is either a table name (str) or a JoinWith object.\n",
    "        If it's a JoinWith object, it's assumed that the table has already \n",
    "        been joined and the fields to remove are in the `remove` \n",
    "        attribute of the object.\n",
    "    :param tables: A mapping of table names to tables (pd.DataFrame)\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "699a8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable, Mapping\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tables = {\n",
    "    \"A\": pd.DataFrame({'b': [1, 2, 3, 33], 'c': [4, 5, 6, 66]}),\n",
    "    \"B\": pd.DataFrame(\n",
    "        {'b': [1, 2, 3], 'a': [4, 5, 6], 'd': [7, 8, 9], \n",
    "         'e': [10, 11, 12], 'f': [13, 14, 15]}\n",
    "    ),\n",
    "    \"C\": pd.DataFrame({'f': [13, 14, 15], 'g': [4, 5, 6]}),\n",
    "    \"D\": pd.DataFrame(\n",
    "        {'d': [7, 8, 77], 'e': [10, 11, 77], 'h': [7, 8, 9], 'i': [1, 2, 3]}\n",
    "    ),\n",
    "    \"E\": pd.DataFrame({'i': [1, 2, 3], 'j': [4, 5, 6]})\n",
    "}\n",
    "\n",
    "field_sets = {table_id: set(df.columns) for table_id, df in tables.items()}\n",
    "assert field_sets == {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class JoinWith:\n",
    "    table_key: str\n",
    "    remove: list = None\n",
    "\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "expected_join_resolution = [\n",
    "    'B',\n",
    "    JoinWith('C', remove=['a', 'f']),\n",
    "    JoinWith('D', remove=['d', 'e', 'h']),\n",
    "    JoinWith('E', remove=['i'])\n",
    "]\n",
    "expected_result = pd.DataFrame({\n",
    "    'b': [1, 2],\n",
    "    'g': [4, 5],\n",
    "    'j': [4, 5]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2d934",
   "metadata": {},
   "source": [
    "The tests would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2b3b95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_join_resolution(\n",
    "        join_resolution: Callable,\n",
    "        *,\n",
    "        field_sets: dict = field_sets, \n",
    "        fields_to_cover: Iterable = fields_to_cover,\n",
    "        expected_join_resolution: list = expected_join_resolution,\n",
    "    ):\n",
    "    assert join_resolution(field_sets, fields_to_cover) == expected_join_resolution\n",
    "\n",
    "def test_compute_join_resolution(\n",
    "    compute_join_resolution: Callable, \n",
    "    *,\n",
    "    resolution_sequence: Iterable = expected_join_resolution,\n",
    "    tables: Mapping[str, pd.DataFrame] = tables,\n",
    "    expected_result: pd.DataFrame = expected_result,   \n",
    "):\n",
    "    result = compute_join_resolution(resolution_sequence, tables)\n",
    "    assert tables_are_equal(result, expected_result)\n",
    "\n",
    "\n",
    "def tables_are_equal(\n",
    "        t1, t2, *, ignore_index=True, sort_index=True, sort_columns=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Compare two tables for equality, ignoring (by default) index and column order.\n",
    "    \"\"\"\n",
    "    if ignore_index:\n",
    "        t1 = t1.reset_index(drop=True)\n",
    "        t2 = t2.reset_index(drop=True)\n",
    "    elif sort_index:\n",
    "        t1 = t1.sort_index(axis=1)\n",
    "        t2 = t2.sort_index(axis=1)\n",
    "    if sort_columns:\n",
    "        t1 = t1.sort_index(axis=0)\n",
    "        t2 = t2.sort_index(axis=0)\n",
    "    return t1.equals(t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0112fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_join_op(obj):\n",
    "    if not isinstance(obj, JoinWith):\n",
    "        return JoinWith(obj)\n",
    "    return obj\n",
    "        \n",
    "\n",
    "def compute_join_resolution(\n",
    "        resolution_sequence: Iterable, tables: Mapping[str, pd.DataFrame]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carries `resolution_sequence` join operations out with tables taken from `tables`.\n",
    "    \n",
    "    :param resolution_sequence: An iterable of join operations to carry out. \n",
    "        Each join operation is either a table name (str) or a JoinWith object.\n",
    "        If it's a JoinWith object, it's assumed that the table has already been joined\n",
    "        and the fields to remove are in the `remove` attribute of the object.\n",
    "    :param tables: A mapping of table names to tables (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    join_ops = map(ensure_join_op, resolution_sequence)\n",
    "    table_key = next(join_ops).table_key\n",
    "    joined = tables[table_key]\n",
    "    for join_op in join_ops:\n",
    "        table = tables[join_op.table_key]\n",
    "        joined = joined.merge(table, how='inner')\n",
    "        if join_op.remove:\n",
    "            remove_cols = set(join_op.remove) & set(joined.columns)\n",
    "            joined = joined.drop(columns=remove_cols)\n",
    "    return joined\n",
    "\n",
    "test_compute_join_resolution(compute_join_resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32fb5b4",
   "metadata": {},
   "source": [
    "**TODO: Implement the join_resolution function -- possibly using the [intersection_graph](https://github.com/i2mint/tabled/blob/282784fc4113b47371ce59a6e14ad2566d7f2abb/tabled/util.py#L25) function on field_sets to help**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd3bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d44a722",
   "metadata": {},
   "source": [
    "## Wip Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ced6884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', JoinWith(table_key='C', remove=[]), JoinWith(table_key='E', remove=[])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_join_resolution(\n",
    "        join_resolution: Callable,\n",
    "        *,\n",
    "        field_sets: dict = field_sets, \n",
    "        fields_to_cover: Iterable = fields_to_cover,\n",
    "        expected_join_resolution: list = expected_join_resolution,\n",
    "    ):\n",
    "    assert join_resolution(field_sets, fields_to_cover) == expected_join_resolution\n",
    "\n",
    "\n",
    "# Given test data and expected results\n",
    "field_sets = {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "\n",
    "\n",
    "def join_resolution(field_sets: dict, fields_to_cover: Iterable[str]) -> list:\n",
    "    fields_to_cover = set(fields_to_cover)\n",
    "    field_sets = field_sets.copy()\n",
    "    resolution_sequence = []\n",
    "    # Initialize covered fields with an empty set\n",
    "    covered_fields = set()\n",
    "\n",
    "    # Loop until we've covered all fields\n",
    "    while not fields_to_cover.issubset(covered_fields):\n",
    "        # Find the table that has the most uncovered fields to cover\n",
    "        next_table = max(\n",
    "            field_sets.keys(),\n",
    "            key=lambda t: len(fields_to_cover.intersection(field_sets[t])) - len(covered_fields.intersection(field_sets[t])),\n",
    "            default=None\n",
    "        )\n",
    "\n",
    "        if next_table is None:\n",
    "            # If we can't find a next table, it means we can't cover the fields with the given field sets\n",
    "            raise ValueError(\"Cannot cover all fields with the given field sets.\")\n",
    "\n",
    "        # Determine which fields from the new table we will need to remove after the join\n",
    "        fields_to_remove = (covered_fields & field_sets[next_table]) - fields_to_cover\n",
    "\n",
    "        # If this is not the first table, we create a JoinWith instance, otherwise, just add the table name\n",
    "        if resolution_sequence:\n",
    "            resolution_sequence.append(JoinWith(table_key=next_table, remove=list(fields_to_remove)))\n",
    "        else:\n",
    "            resolution_sequence.append(next_table)\n",
    "\n",
    "        # Update covered fields\n",
    "        covered_fields.update(field_sets[next_table])\n",
    "\n",
    "        # Remove the used table from field_sets to prevent reusing it\n",
    "        del field_sets[next_table]\n",
    "\n",
    "    return resolution_sequence\n",
    "\n",
    "# test_join_resolution(join_resolution)\n",
    "join_resolution(field_sets, fields_to_cover)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8480d4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'B')}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given test data and expected results\n",
    "field_sets = {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "\n",
    "def test_minimum_covering_intersections(\n",
    "        minimum_covering_intersections: Callable,\n",
    "        *,\n",
    "        field_sets: dict = field_sets, \n",
    "        fields_to_cover: Iterable = fields_to_cover,\n",
    "        expected_result: set = {('B', 'C'), ('B', 'D'), ('B', 'E')}\n",
    "    ):\n",
    "    assert minimum_covering_intersections(field_sets, fields_to_cover) == expected_result\n",
    "\n",
    "\n",
    "def minimum_covering_intersections(field_sets, fields_to_cover):\n",
    "    \"\"\"Returns a set of set pairs that connect the elements of `fields_to_cover`.\n",
    "    \n",
    "    :param field_sets: A mapping of sets of field names\n",
    "    :param fields_to_cover: A set of field names to cover\n",
    "    :return: A set of set pairs that connect the elements of `fields_to_cover`\n",
    "    \n",
    "    The returned pairs are `(key_1, key_2)` pairs of keys of `field_sets`\n",
    "    such that field_sets[key_1] & field_sets[key_2] is not empty.\n",
    "    The union of all the sets in the pairs should cover `fields_to_cover`.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "def minimum_covering_intersections(field_sets, fields_to_cover):\n",
    "    fields_to_cover = set(fields_to_cover)  # Ensure we have a set to work with\n",
    "    covering_pairs = set()\n",
    "    covered_fields = set()\n",
    "\n",
    "    # Go through all unique pairs of keys from field_sets\n",
    "    for key1, key2 in combinations(field_sets.keys(), 2):\n",
    "        intersection = field_sets[key1] & field_sets[key2]\n",
    "        \n",
    "        # Check if this pair covers any new fields that we need to cover\n",
    "        newly_covered = intersection & fields_to_cover - covered_fields\n",
    "        if newly_covered:\n",
    "            covering_pairs.add((key1, key2))\n",
    "            covered_fields.update(newly_covered)\n",
    "            \n",
    "        # If we've covered all fields, we can stop early\n",
    "        if covered_fields == fields_to_cover:\n",
    "            break\n",
    "\n",
    "    return covering_pairs\n",
    "\n",
    "minimum_covering_intersections(field_sets, fields_to_cover)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2b34823d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {'b', 'c'},\n",
       " 'B': {'a', 'b', 'd', 'e', 'f'},\n",
       " 'C': {'f', 'g'},\n",
       " 'D': {'d', 'e', 'h', 'i'},\n",
       " 'E': {'i', 'j'}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "329b9eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabled.util import intersection_graph\n",
    "\n",
    "assert intersection_graph(field_sets) == {\n",
    "    'A': {'B'}, 'B': {'A', 'C', 'D'}, 'C': {'B'}, 'D': {'B', 'E'}, 'E': {'D'}\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17584ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Tuple, Set\n",
    "\n",
    "def minimum_covering_intersections(field_sets: Dict[str, Set[str]], fields_to_cover: Set[str]) -> Set[Tuple[str, str]]:\n",
    "    # Construct the intersection graph\n",
    "    graph = intersection_graph(field_sets, edge_labels='elements')\n",
    "    \n",
    "    # Initialize variables to track covered fields and the pairs to return\n",
    "    covered_fields = set()\n",
    "    covering_pairs = set()\n",
    "\n",
    "    # Function to perform Depth-First Search (DFS) on the graph\n",
    "    def dfs(node, visited, component):\n",
    "        visited.add(node)\n",
    "        component.add(node)\n",
    "        for neighbour in graph[node]:\n",
    "            if neighbour not in visited:\n",
    "                dfs(neighbour, visited, component)\n",
    "                \n",
    "    # Find all connected components in the graph\n",
    "    visited = set()\n",
    "    components = []\n",
    "    for node in graph:\n",
    "        if node not in visited:\n",
    "            component = set()\n",
    "            dfs(node, visited, component)\n",
    "            components.append(component)\n",
    "    \n",
    "    # Find the minimum set of set pairs for each component\n",
    "    for component in components:\n",
    "        # Check if this component contains any of the fields we need to cover\n",
    "        if any(field in covered_fields for node in component for field in field_sets[node]):\n",
    "            # Perform a modified Prim's algorithm to find the minimum spanning tree that covers fields_to_cover\n",
    "            mst_edges = set()  # This will store the edges of the minimum spanning tree\n",
    "            component_fields_to_cover = {field for node in component for field in field_sets[node] if field in fields_to_cover}\n",
    "            covered_in_component = set()\n",
    "            available_edges = [(len(graph[node][neighbour]), node, neighbour) for node in component for neighbour in graph[node] if neighbour in component]\n",
    "            sorted_edges = sorted(available_edges)\n",
    "            \n",
    "            while component_fields_to_cover - covered_in_component:\n",
    "                # Select the edge with minimum weight (smallest intersection)\n",
    "                _, node1, node2 = sorted_edges.pop(0)\n",
    "                # Add this edge if it helps to cover more fields\n",
    "                if field_sets[node1] & field_sets[node2] & fields_to_cover:\n",
    "                    mst_edges.add((node1, node2))\n",
    "                    covered_in_component.update(field_sets[node1] & field_sets[node2])\n",
    "            \n",
    "            # Add the edges of this minimum spanning tree to the covering pairs\n",
    "            covering_pairs.update(mst_edges)\n",
    "    \n",
    "    # Make sure the pairs are in the correct order\n",
    "    ordered_covering_pairs = {(min(pair), max(pair)) for pair in covering_pairs}\n",
    "    \n",
    "    return ordered_covering_pairs\n",
    "\n",
    "\n",
    "field_sets = {'A': {'b', 'c'},\n",
    " 'B': {'a', 'b', 'd', 'e', 'f'},\n",
    " 'C': {'f', 'g'},\n",
    " 'D': {'d', 'e', 'h', 'i'},\n",
    " 'E': {'i', 'j'}}\n",
    "\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "\n",
    "edges = minimum_covering_intersections(field_sets, fields_to_cover)\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0b8683f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m edges \u001b[38;5;241m=\u001b[39m minimum_covering_intersections(field_sets, fields_to_cover)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(edges) \u001b[38;5;241m==\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "edges = minimum_covering_intersections(field_sets, fields_to_cover)\n",
    "assert sorted(edges) == [('B', 'C'), ('B', 'D'), ('D', 'E')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74063324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e32a1d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " JoinWith(table_key='C', remove=['f', 'c', 'b']),\n",
       " JoinWith(table_key='E', remove=['g', 'i', 'f', 'b', 'c'])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterable, Mapping\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "# Define the JoinWith dataclass\n",
    "@dataclass\n",
    "class JoinWith:\n",
    "    table_key: str\n",
    "    remove: list = None\n",
    "\n",
    "# Define the function to find the join resolution\n",
    "def join_resolution(field_sets: dict, fields_to_cover: Iterable[str]) -> list:\n",
    "    fields_to_cover = set(fields_to_cover)  # Set of fields we need to cover\n",
    "    join_sequence = []  # The output sequence of joins\n",
    "    covered_fields = set()  # Fields covered by the join sequence so far\n",
    "\n",
    "    # While there are fields left to cover, iterate\n",
    "    while fields_to_cover:\n",
    "        # Find the table that covers the most fields which are not yet covered\n",
    "        next_table, next_fields = max(\n",
    "            field_sets.items(),\n",
    "            key=lambda item: len(fields_to_cover.intersection(item[1])) - len(covered_fields.intersection(item[1])),\n",
    "            default=(None, None)\n",
    "        )\n",
    "\n",
    "        # If no table can cover any more fields, break the loop\n",
    "        if not next_table:\n",
    "            break\n",
    "\n",
    "        # Determine which fields we will need to remove after joining this table\n",
    "        fields_to_remove = (covered_fields | next_fields) - fields_to_cover\n",
    "\n",
    "        # Add the next table to the join sequence\n",
    "        if join_sequence:\n",
    "            join_sequence.append(JoinWith(next_table, remove=list(fields_to_remove)))\n",
    "        else:  # If this is the first table, we don't need to remove any fields\n",
    "            join_sequence.append(next_table)\n",
    "\n",
    "        # Update the sets of covered fields and the remaining fields to cover\n",
    "        covered_fields.update(next_fields)\n",
    "        fields_to_cover -= next_fields\n",
    "\n",
    "        # Remove the chosen table from field_sets to avoid re-selecting it\n",
    "        del field_sets[next_table]\n",
    "\n",
    "    # Ensure that the last operation does not include a removal of fields from the last table joined\n",
    "    if join_sequence and isinstance(join_sequence[-1], JoinWith):\n",
    "        join_sequence[-1].remove = [f for f in join_sequence[-1].remove if f in covered_fields - fields_to_cover]\n",
    "\n",
    "    return join_sequence\n",
    "\n",
    "# Given test data and expected results\n",
    "field_sets = {\n",
    "    \"A\": {'b', 'c'},\n",
    "    \"B\": {'b', 'a', 'd', 'e', 'f'},\n",
    "    \"C\": {'f', 'g'},\n",
    "    \"D\": {'d', 'e', 'h', 'i'},\n",
    "    \"E\": {'i', 'j'}\n",
    "}\n",
    "fields_to_cover = ['b', 'g', 'j']\n",
    "\n",
    "# Expected output\n",
    "expected_join_resolution = [\n",
    "    'B',\n",
    "    JoinWith('C', remove=['a', 'f']),\n",
    "    JoinWith('D', remove=['d', 'e', 'h']),\n",
    "    JoinWith('E', remove=['i'])\n",
    "]\n",
    "\n",
    "# Run the function with the test data\n",
    "join_resolution_output = join_resolution(field_sets.copy(), fields_to_cover)\n",
    "\n",
    "# Display the output\n",
    "join_resolution_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017edbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c5715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b8c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f1848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc323b3e",
   "metadata": {},
   "source": [
    "# Proposal: tiny light framework for routing with mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0acf755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T12:07:39.913553Z",
     "start_time": "2023-02-06T12:07:39.875060Z"
    }
   },
   "outputs": [],
   "source": [
    "from i2.routing_forest import KeyFuncMapping\n",
    "from tabled import dflt_ext_mapping, get_ext\n",
    "\n",
    "from dol import Files, FilesOfZip, Pipe, wrap_kvs\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import io\n",
    "\n",
    "ext = KeyFuncMapping(\n",
    "    dict(\n",
    "        dflt_ext_mapping, \n",
    "        json=Pipe(io.BytesIO, partial(pd.read_json, orient='index'))\n",
    "    ),\n",
    "    get_ext,\n",
    ")\n",
    "\n",
    "table_trans = wrap_kvs(postget=lambda k, v: ext[k](v))\n",
    "\n",
    "TableFiles = table_trans(Files)\n",
    "TableZipFiles = table_trans(FilesOfZip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cacd8921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T12:07:44.075672Z",
     "start_time": "2023-02-06T12:07:43.973256Z"
    }
   },
   "outputs": [],
   "source": [
    "s = TableZipFiles('/Users/thorwhalen/Dropbox/_odata/sound/induction_motor_data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d016bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T12:07:45.572475Z",
     "start_time": "2023-02-06T12:07:45.547063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3909"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3d28ae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T14:50:43.104488Z",
     "start_time": "2023-02-03T14:50:43.053751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k='induction_motor_data/Bearing/Bearing_1250rpm/2021_01_27_15_03_06.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dataType</th>\n",
       "      <td>completeSample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deviceId</th>\n",
       "      <td>00000781O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flux</th>\n",
       "      <td>[31.3639, 23.8975, 35.5119, 27.0085, 16.6385, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motorId</th>\n",
       "      <td>adi#0c7dbd85-a5e7-4e6a-b37c-a49e1adaca45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempe</th>\n",
       "      <td>[25.875, 25.875]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempm</th>\n",
       "      <td>[25.9375, 25.9375]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tenantId</th>\n",
       "      <td>adi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <td>1611756186321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <td>60814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsr</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vbat</th>\n",
       "      <td>[3.14817659, 3.130985182, 3.122389478, 3.14817...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibx</th>\n",
       "      <td>[-0.15531, -0.48733000000000004, 0.84075000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibz</th>\n",
       "      <td>[-2.41427, -0.42215, -1.08619, -0.256140000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0\n",
       "dataType                                      completeSample\n",
       "deviceId                                           00000781O\n",
       "flux       [31.3639, 23.8975, 35.5119, 27.0085, 16.6385, ...\n",
       "motorId             adi#0c7dbd85-a5e7-4e6a-b37c-a49e1adaca45\n",
       "tempe                                       [25.875, 25.875]\n",
       "tempm                                     [25.9375, 25.9375]\n",
       "tenantId                                                 adi\n",
       "timestamp                                      1611756186321\n",
       "ts                                                     60814\n",
       "tsr                                                    False\n",
       "vbat       [3.14817659, 3.130985182, 3.122389478, 3.14817...\n",
       "vibx       [-0.15531, -0.48733000000000004, 0.84075000000...\n",
       "vibz       [-2.41427, -0.42215, -1.08619, -0.256140000000..."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k, v = s.head()\n",
    "print(f\"{k=}\")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad2dcd4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:19:33.389611Z",
     "start_time": "2023-02-08T13:19:32.954311Z"
    }
   },
   "outputs": [],
   "source": [
    "from hubcap import GitHubReader, Github\n",
    "\n",
    "\n",
    "t = GitHubReader('i2mint')\n",
    "tt = list(t);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83a95597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:40:42.966502Z",
     "start_time": "2023-02-08T13:40:42.942188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[aiofiledol](https://www.github.com/i2mint/aiofiledol)\n",
      "    * [arangodol](https://www.github.com/i2mint/arangodol)\n",
      "    * [couchdol](https://www.github.com/i2mint/couchdol)\n",
      "    * [dol](https://www.github.com/i2mint/dol)\n",
      "    * [dropboxdol](https://www.github.com/i2mint/dropboxdol)\n",
      "    * [dynamodol](https://www.github.com/i2mint/dynamodol)\n",
      "    * [ftpdol](https://www.github.com/i2mint/ftpdol)\n",
      "    * [mongodol](https://www.github.com/i2mint/mongodol)\n",
      "    * [odbcdol](https://www.github.com/i2mint/odbcdol)\n",
      "    * [pydrivedol](https://www.github.com/i2mint/pydrivedol)\n",
      "    * [redisdol](https://www.github.com/i2mint/redisdol)\n",
      "    * [s3dol](https://www.github.com/i2mint/s3dol)\n",
      "    * [sqldol](https://www.github.com/i2mint/sqldol)\n",
      "    * [sshdol](https://www.github.com/i2mint/sshdol)\n"
     ]
    }
   ],
   "source": [
    "w = filter(lambda x: x.endswith('dol'), tt)\n",
    "w = map(lambda x: f\"[{x}](https://www.github.com/i2mint/{x})\", w)\n",
    "print(*w, sep='\\n    * ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1b1aad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:55:21.212751Z",
     "start_time": "2023-02-08T13:55:20.755353Z"
    }
   },
   "outputs": [],
   "source": [
    "from redisdol import RedisBytesPersister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39bb6a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:58:03.557941Z",
     "start_time": "2023-02-08T13:58:03.531262Z"
    }
   },
   "outputs": [],
   "source": [
    "s = RedisBytesPersister()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3d5c7c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T13:58:05.964609Z",
     "start_time": "2023-02-08T13:58:05.928849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec242408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:00:04.123079Z",
     "start_time": "2023-02-09T09:00:04.114326Z"
    }
   },
   "outputs": [],
   "source": [
    "t = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bbf847b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:00:12.277889Z",
     "start_time": "2023-02-09T09:00:12.255548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t += [5, 6]\n",
    "# t.extend([5, 6])\n",
    "t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63a6fb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:00:19.100109Z",
     "start_time": "2023-02-09T09:00:19.092488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 8]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.append(8)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721bb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[k].append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20e6fd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:01:48.504289Z",
     "start_time": "2023-02-09T09:01:48.227157Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pwd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Files\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpwd\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pwd' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91119a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963eab94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817918b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e807f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79810cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14dde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0530775d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
